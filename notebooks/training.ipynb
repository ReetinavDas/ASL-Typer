{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils as utils\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [chr(ord) for ord in range(65,91)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes.append(\"Space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = tuple(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "hands = mp_hands.Hands(min_detection_confidence=0.7, static_image_mode=True, max_num_hands=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5996it [04:07, 24.21it/s]\n",
      "5996it [04:17, 23.33it/s]\n",
      "5996it [04:09, 24.02it/s]\n",
      "5996it [03:39, 27.34it/s]\n",
      "5996it [04:29, 22.23it/s]\n",
      "5996it [04:19, 23.06it/s]\n",
      "5996it [04:08, 24.09it/s]\n",
      "5996it [04:11, 23.82it/s]\n",
      "5996it [04:11, 23.80it/s]\n",
      "5996it [04:08, 24.12it/s]\n",
      "5996it [07:04, 14.14it/s]\n",
      "5996it [04:07, 24.20it/s]\n",
      "5996it [15:02,  6.64it/s]\n",
      "5996it [16:29,  6.06it/s]\n",
      "5996it [24:09,  4.14it/s]\n",
      "5996it [46:27,  2.15it/s] \n",
      "5996it [24:41,  4.05it/s] \n",
      "5966it [04:12, 23.59it/s]\n",
      "5996it [04:09, 24.04it/s]\n",
      "5648it [03:58, 23.70it/s]\n",
      "4542it [03:11, 23.77it/s]\n",
      "5996it [04:15, 23.46it/s]\n",
      "5996it [04:09, 24.01it/s]\n",
      "5996it [04:09, 24.04it/s]\n",
      "5720it [03:59, 23.86it/s]\n",
      "5996it [04:06, 24.34it/s]\n",
      "5886it [03:27, 28.37it/s]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "for class_index, gesture_class in enumerate(classes):\n",
    "    for i, filename in tqdm(enumerate(os.listdir(f\"../data/train/{gesture_class}\"))):\n",
    "        try:\n",
    "            image = cv2.imread(f\"../data/train/{gesture_class}/{filename}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "        except:\n",
    "            continue\n",
    "        image.flags.writeable = False\n",
    "        if i%2 == 0:\n",
    "            image = cv2.flip(image, 1)\n",
    "        results = hands.process(image) # this makes the actual detections\n",
    "\n",
    "        landmarks = []\n",
    "        if results.multi_hand_landmarks:\n",
    "            for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "                x, y = landmark.x, landmark.y\n",
    "                landmarks.append([x,y])\n",
    "            label = np.zeros([len(classes)])\n",
    "            label[class_index] = 1\n",
    "            data.append(landmarks)\n",
    "            labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(data)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([146554, 21, 2])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"../data/data.pkl\", 'wb') as file:\n",
    "#     pickle.dump(data, file)\n",
    "# with open(\"../data/labels.pkl\", 'wb') as file:\n",
    "#     pickle.dump(labels, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, train_labels, val_labels = train_test_split(data, labels, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "hands = mp_hands.Hands(min_detection_confidence=0.001, static_image_mode=True, max_num_hands=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Items: 27 classes [00:05,  5.26 classes/s]\n"
     ]
    }
   ],
   "source": [
    "test_data = []\n",
    "test_labels = []\n",
    "\n",
    "for class_index, gesture_class in tqdm(enumerate(classes), unit=\" classes\", desc=\"Processing Items\"):\n",
    "    for filename in os.listdir(f\"../data/test/{gesture_class}\"):\n",
    "        try:\n",
    "            image = cv2.imread(f\"../data/test/{gesture_class}/{filename}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "        except:\n",
    "            continue\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image) # this makes the actual detections\n",
    "\n",
    "        landmarks = []\n",
    "        if results.multi_hand_landmarks:\n",
    "            for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "                x, y = landmark.x, landmark.y\n",
    "                landmarks.append([x,y])\n",
    "            test_label = np.zeros([len(classes)])\n",
    "            test_label[class_index] = 1\n",
    "            test_data.append(landmarks)\n",
    "            test_labels.append(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.tensor(test_data)\n",
    "test_labels = torch.tensor(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarksDataset(utils.data.Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.len = len(y)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = LandmarksDataset(train_data, train_labels)\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = LandmarksDataset(val_data, val_labels)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = LandmarksDataset(test_data, test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HandNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(42, 120)\n",
    "        self.fc2 = nn.Linear(120, 100)\n",
    "        self.fc3 = nn.Linear(100, 100)\n",
    "        self.fc4 = nn.Linear(100, len(classes))\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HandNetwork()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(curr_model):\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "\n",
    "    for i, data in enumerate(training_loader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = curr_model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward() # calculate the gradients\n",
    "        optimizer.step() # update the params\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 500 == 500-1:\n",
    "            last_loss = running_loss / 500 # loss per batch\n",
    "            print(f'  batch {i+1} loss: {last_loss}')\n",
    "            running_loss = 0\n",
    "    \n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 500 loss: 3.291951558709145\n",
      "  batch 1000 loss: 3.2854449266195296\n",
      "  batch 1500 loss: 3.2798778759241105\n",
      "  batch 2000 loss: 3.271049015760422\n",
      "  batch 2500 loss: 3.2524211324453356\n",
      "  batch 3000 loss: 3.226501616001129\n",
      "  batch 3500 loss: 3.135888165652752\n",
      "  batch 4000 loss: 2.9161784588098527\n",
      "  batch 4500 loss: 2.5723186208680273\n",
      "  batch 5000 loss: 2.1946498931869867\n",
      "  batch 5500 loss: 1.7133658626228572\n",
      "  batch 6000 loss: 1.3702675198614598\n",
      "  batch 6500 loss: 1.2396998058967292\n",
      "  batch 7000 loss: 1.1105683126032817\n",
      "  batch 7500 loss: 1.0534648515495937\n",
      "  batch 8000 loss: 0.9416265707058483\n",
      "  batch 8500 loss: 0.959827609001426\n",
      "  batch 9000 loss: 0.8484199686181965\n",
      "  batch 9500 loss: 0.7025062396142748\n",
      "  batch 10000 loss: 0.6788454892197624\n",
      "  batch 10500 loss: 0.5662436739135446\n",
      "  batch 11000 loss: 0.5305625099634345\n",
      "  batch 11500 loss: 0.5177961312113112\n",
      "  batch 12000 loss: 0.5214467175775354\n",
      "  batch 12500 loss: 0.4459445121842291\n",
      "  batch 13000 loss: 0.4329548508489643\n",
      "  batch 13500 loss: 0.44778366088252597\n",
      "  batch 14000 loss: 0.4221225045811989\n",
      "  batch 14500 loss: 0.3839805590372125\n",
      "  batch 15000 loss: 0.35919420752529096\n",
      "  batch 15500 loss: 0.3752235036563834\n",
      "  batch 16000 loss: 0.36562771536945365\n",
      "  batch 16500 loss: 0.35710628267328504\n",
      "  batch 17000 loss: 0.37272081033983523\n",
      "  batch 17500 loss: 0.3416933079504779\n",
      "  batch 18000 loss: 0.28482599523956376\n",
      "  batch 18500 loss: 0.3775142064325028\n",
      "  batch 19000 loss: 0.3335079695760851\n",
      "  batch 19500 loss: 0.2780338261765137\n",
      "  batch 20000 loss: 0.3628517519037114\n",
      "  batch 20500 loss: 0.2786266023372718\n",
      "  batch 21000 loss: 0.27277841091544247\n",
      "  batch 21500 loss: 0.2812748849437339\n",
      "  batch 22000 loss: 0.26525788877488593\n",
      "  batch 22500 loss: 0.27490751176208594\n",
      "  batch 23000 loss: 0.28801852593613014\n",
      "  batch 23500 loss: 0.28167798907783526\n",
      "  batch 24000 loss: 0.25654965044898015\n",
      "  batch 24500 loss: 0.21415527468185838\n",
      "  batch 25000 loss: 0.2830366136158127\n",
      "  batch 25500 loss: 0.257388265870155\n",
      "  batch 26000 loss: 0.23285431717690164\n",
      "  batch 26500 loss: 0.25483323577540523\n",
      "  batch 27000 loss: 0.24063508663328514\n",
      "LOSS train 0.24063508663328514 valid 0.16048274615026445\n",
      "EPOCH 2:\n",
      "  batch 500 loss: 0.24883536319798027\n",
      "  batch 1000 loss: 0.22100525316991002\n",
      "  batch 1500 loss: 0.22526047071202038\n",
      "  batch 2000 loss: 0.25179737050729367\n",
      "  batch 2500 loss: 0.17855138706783089\n",
      "  batch 3000 loss: 0.2191953035681477\n",
      "  batch 3500 loss: 0.18396055076889933\n",
      "  batch 4000 loss: 0.22201815332497707\n",
      "  batch 4500 loss: 0.22468572145325766\n",
      "  batch 5000 loss: 0.18396383378862485\n",
      "  batch 5500 loss: 0.19120880833137074\n",
      "  batch 6000 loss: 0.17541659417538785\n",
      "  batch 6500 loss: 0.19569482942906735\n",
      "  batch 7000 loss: 0.1689665311266029\n",
      "  batch 7500 loss: 0.17529476415194092\n",
      "  batch 8000 loss: 0.1751191433945032\n",
      "  batch 8500 loss: 0.15944412952012335\n",
      "  batch 9000 loss: 0.16891524781594325\n",
      "  batch 9500 loss: 0.21066538566166393\n",
      "  batch 10000 loss: 0.14880248833739956\n",
      "  batch 10500 loss: 0.1464361744741601\n",
      "  batch 11000 loss: 0.16950870782598243\n",
      "  batch 11500 loss: 0.14514936343917678\n",
      "  batch 12000 loss: 0.1302543441485053\n",
      "  batch 12500 loss: 0.14092581107922433\n",
      "  batch 13000 loss: 0.121726574677293\n",
      "  batch 13500 loss: 0.09959569084011957\n",
      "  batch 14000 loss: 0.1417516810153511\n",
      "  batch 14500 loss: 0.11366796198989153\n",
      "  batch 15000 loss: 0.14994991579347608\n",
      "  batch 15500 loss: 0.11601702581510352\n",
      "  batch 16000 loss: 0.1290050149683356\n",
      "  batch 16500 loss: 0.11234413347430483\n",
      "  batch 17000 loss: 0.12074266349410334\n",
      "  batch 17500 loss: 0.12957650433465084\n",
      "  batch 18000 loss: 0.11247375996065102\n",
      "  batch 18500 loss: 0.10519010448717347\n",
      "  batch 19000 loss: 0.08104832231747375\n",
      "  batch 19500 loss: 0.12887334066290676\n",
      "  batch 20000 loss: 0.10270842403695667\n",
      "  batch 20500 loss: 0.09879845544131186\n",
      "  batch 21000 loss: 0.08535903924226215\n",
      "  batch 21500 loss: 0.0865923969115925\n",
      "  batch 22000 loss: 0.09778923835815527\n",
      "  batch 22500 loss: 0.08579841065441544\n",
      "  batch 23000 loss: 0.11216880299527424\n",
      "  batch 23500 loss: 0.13785044475947028\n",
      "  batch 24000 loss: 0.06754016640395967\n",
      "  batch 24500 loss: 0.0694669913328471\n",
      "  batch 25000 loss: 0.07453457886633408\n",
      "  batch 25500 loss: 0.08479537411528998\n",
      "  batch 26000 loss: 0.0865826612757752\n",
      "  batch 26500 loss: 0.09822745103560754\n",
      "  batch 27000 loss: 0.0780903081494877\n",
      "LOSS train 0.0780903081494877 valid 0.05731207125665505\n",
      "EPOCH 3:\n",
      "  batch 500 loss: 0.054924314304767394\n",
      "  batch 1000 loss: 0.10130844933760186\n",
      "  batch 1500 loss: 0.061422847022188594\n",
      "  batch 2000 loss: 0.06977047647399855\n",
      "  batch 2500 loss: 0.07487571991033272\n",
      "  batch 3000 loss: 0.06886369533990702\n",
      "  batch 3500 loss: 0.09380645301830452\n",
      "  batch 4000 loss: 0.06875765738589583\n",
      "  batch 4500 loss: 0.05151400641105977\n",
      "  batch 5000 loss: 0.05770185971734196\n",
      "  batch 5500 loss: 0.05580131784743704\n",
      "  batch 6000 loss: 0.05520639734756476\n",
      "  batch 6500 loss: 0.05364352768400977\n",
      "  batch 7000 loss: 0.05063836745903579\n",
      "  batch 7500 loss: 0.06945226873724542\n",
      "  batch 8000 loss: 0.04503081727791765\n",
      "  batch 8500 loss: 0.07880478536182015\n",
      "  batch 9000 loss: 0.04945257553385146\n",
      "  batch 9500 loss: 0.05950872181033054\n",
      "  batch 10000 loss: 0.06149472353034074\n",
      "  batch 10500 loss: 0.03228084752792118\n",
      "  batch 11000 loss: 0.044957002877449526\n",
      "  batch 11500 loss: 0.04768884196038431\n",
      "  batch 12000 loss: 0.04872174156999051\n",
      "  batch 12500 loss: 0.0411590279513745\n",
      "  batch 13000 loss: 0.06702856252119012\n",
      "  batch 13500 loss: 0.060432745463530876\n",
      "  batch 14000 loss: 0.023458069165334307\n",
      "  batch 14500 loss: 0.08145592542734971\n",
      "  batch 15000 loss: 0.022673727643261372\n",
      "  batch 15500 loss: 0.06571613175175825\n",
      "  batch 16000 loss: 0.05459308239472551\n",
      "  batch 16500 loss: 0.08274860359509374\n",
      "  batch 17000 loss: 0.04872973298823592\n",
      "  batch 17500 loss: 0.03895671814217371\n",
      "  batch 18000 loss: 0.026106199939612898\n",
      "  batch 18500 loss: 0.031018194600994604\n",
      "  batch 19000 loss: 0.05786312866199772\n",
      "  batch 19500 loss: 0.034057359095944845\n",
      "  batch 20000 loss: 0.05807488791829619\n",
      "  batch 20500 loss: 0.03194130463818914\n",
      "  batch 21000 loss: 0.03129787123180597\n",
      "  batch 21500 loss: 0.0628707816572781\n",
      "  batch 22000 loss: 0.05709494272313701\n",
      "  batch 22500 loss: 0.0360738317881774\n",
      "  batch 23000 loss: 0.04137288003793293\n",
      "  batch 23500 loss: 0.048418566745284865\n",
      "  batch 24000 loss: 0.06158316860407122\n",
      "  batch 24500 loss: 0.028247641408880916\n",
      "  batch 25000 loss: 0.017823672691556294\n",
      "  batch 25500 loss: 0.0665662136426127\n",
      "  batch 26000 loss: 0.030736473789879308\n",
      "  batch 26500 loss: 0.029388961613662258\n",
      "  batch 27000 loss: 0.03464116324772725\n",
      "LOSS train 0.03464116324772725 valid 0.11174350685569719\n",
      "EPOCH 4:\n",
      "  batch 500 loss: 0.018033673051319424\n",
      "  batch 1000 loss: 0.01725853391410388\n",
      "  batch 1500 loss: 0.029241814053960223\n",
      "  batch 2000 loss: 0.057165708142493256\n",
      "  batch 2500 loss: 0.032913673506036814\n",
      "  batch 3000 loss: 0.023783512567813914\n",
      "  batch 3500 loss: 0.031130603523212813\n",
      "  batch 4000 loss: 0.030458535369857924\n",
      "  batch 4500 loss: 0.044351329897914585\n",
      "  batch 5000 loss: 0.04527486323774764\n",
      "  batch 5500 loss: 0.03683245380908586\n",
      "  batch 6000 loss: 0.04861163826753851\n",
      "  batch 6500 loss: 0.021575009255272205\n",
      "  batch 7000 loss: 0.024205456459641683\n",
      "  batch 7500 loss: 0.02896819892260918\n",
      "  batch 8000 loss: 0.027178765163020913\n",
      "  batch 8500 loss: 0.03881798680418625\n",
      "  batch 9000 loss: 0.018659446687585998\n",
      "  batch 9500 loss: 0.033333535527764295\n",
      "  batch 10000 loss: 0.017523701257158575\n",
      "  batch 10500 loss: 0.01738881499593201\n",
      "  batch 11000 loss: 0.04789982596085872\n",
      "  batch 11500 loss: 0.01857078988006774\n",
      "  batch 12000 loss: 0.026854327805326053\n",
      "  batch 12500 loss: 0.01570033352360183\n",
      "  batch 13000 loss: 0.015794634398054307\n",
      "  batch 13500 loss: 0.04514015828992052\n",
      "  batch 14000 loss: 0.0227603211449478\n",
      "  batch 14500 loss: 0.020110719442824315\n",
      "  batch 15000 loss: 0.0529304013310541\n",
      "  batch 15500 loss: 0.02818228117152843\n",
      "  batch 16000 loss: 0.014443773001296392\n",
      "  batch 16500 loss: 0.0262579331950751\n",
      "  batch 17000 loss: 0.01135379204383703\n",
      "  batch 17500 loss: 0.042158249827426726\n",
      "  batch 18000 loss: 0.0534449544747408\n",
      "  batch 18500 loss: 0.03195904886294033\n",
      "  batch 19000 loss: 0.03288068796300067\n",
      "  batch 19500 loss: 0.03860193716896271\n",
      "  batch 20000 loss: 0.04559431948902181\n",
      "  batch 20500 loss: 0.014526178664257795\n",
      "  batch 21000 loss: 0.021006910083231627\n",
      "  batch 21500 loss: 0.03352459825166778\n",
      "  batch 22000 loss: 0.0553551372951676\n",
      "  batch 22500 loss: 0.01981133817607625\n",
      "  batch 23000 loss: 0.021155384727300747\n",
      "  batch 23500 loss: 0.03421403144089817\n",
      "  batch 24000 loss: 0.041267741964694835\n",
      "  batch 24500 loss: 0.01874707911376972\n",
      "  batch 25000 loss: 0.041779960363767105\n",
      "  batch 25500 loss: 0.029020720454673268\n",
      "  batch 26000 loss: 0.010638347084059273\n",
      "  batch 26500 loss: 0.029694427957374585\n",
      "  batch 27000 loss: 0.020914274251441132\n",
      "LOSS train 0.020914274251441132 valid 0.010882955024104335\n",
      "EPOCH 5:\n",
      "  batch 500 loss: 0.01029849033829381\n",
      "  batch 1000 loss: 0.029620526646842296\n",
      "  batch 1500 loss: 0.016559551819051915\n",
      "  batch 2000 loss: 0.006747412602063697\n",
      "  batch 2500 loss: 0.03486549759431216\n",
      "  batch 3000 loss: 0.023533114558885754\n",
      "  batch 3500 loss: 0.026691244874877028\n",
      "  batch 4000 loss: 0.018684073024482058\n",
      "  batch 4500 loss: 0.02440057526772773\n",
      "  batch 5000 loss: 0.012501337745797822\n",
      "  batch 5500 loss: 0.026068780429213286\n",
      "  batch 6000 loss: 0.04625097510091653\n",
      "  batch 6500 loss: 0.03865690954178883\n",
      "  batch 7000 loss: 0.04233388385991187\n",
      "  batch 7500 loss: 0.012658054357854344\n",
      "  batch 8000 loss: 0.011833282587875126\n",
      "  batch 8500 loss: 0.009807579061310787\n",
      "  batch 9000 loss: 0.015287569766936961\n",
      "  batch 9500 loss: 0.025515434631410334\n",
      "  batch 10000 loss: 0.032145733192287335\n",
      "  batch 10500 loss: 0.037138879475859594\n",
      "  batch 11000 loss: 0.03213590714975546\n",
      "  batch 11500 loss: 0.01831184364804336\n",
      "  batch 12000 loss: 0.012388486031790151\n",
      "  batch 12500 loss: 0.029920816133861564\n",
      "  batch 13000 loss: 0.02823886628138864\n",
      "  batch 13500 loss: 0.02150261164693165\n",
      "  batch 14000 loss: 0.012618219216015348\n",
      "  batch 14500 loss: 0.02156051322356663\n",
      "  batch 15000 loss: 0.029409423828605837\n",
      "  batch 15500 loss: 0.017858520670804446\n",
      "  batch 16000 loss: 0.011300379184423828\n",
      "  batch 16500 loss: 0.013408544581214165\n",
      "  batch 17000 loss: 0.02494413316118589\n",
      "  batch 17500 loss: 0.014795523157007282\n",
      "  batch 18000 loss: 0.013836895888768623\n",
      "  batch 18500 loss: 0.023621264400851934\n",
      "  batch 19000 loss: 0.025594321277219577\n",
      "  batch 19500 loss: 0.014446713358630636\n",
      "  batch 20000 loss: 0.02645172512133245\n",
      "  batch 20500 loss: 0.007156408953306301\n",
      "  batch 21000 loss: 0.01130510043345857\n",
      "  batch 21500 loss: 0.005555758020584985\n",
      "  batch 22000 loss: 0.012933464192958493\n",
      "  batch 22500 loss: 0.02559191816697486\n",
      "  batch 23000 loss: 0.026092583956028627\n",
      "  batch 23500 loss: 0.006407292340415413\n",
      "  batch 24000 loss: 0.011679332889834746\n",
      "  batch 24500 loss: 0.004043419035224709\n",
      "  batch 25000 loss: 0.01732001895450067\n",
      "  batch 25500 loss: 0.005940993188940024\n",
      "  batch 26000 loss: 0.014809197678066561\n",
      "  batch 26500 loss: 0.0023528087732967115\n",
      "  batch 27000 loss: 0.02370397525247285\n",
      "LOSS train 0.02370397525247285 valid 0.007327539538416602\n",
      "EPOCH 6:\n",
      "  batch 500 loss: 0.007871996803527723\n",
      "  batch 1000 loss: 0.020021838146741158\n",
      "  batch 1500 loss: 0.006777481601995899\n",
      "  batch 2000 loss: 0.03034551096706303\n",
      "  batch 2500 loss: 0.021289220164220378\n",
      "  batch 3000 loss: 0.008799074453281886\n",
      "  batch 3500 loss: 0.004919507081207833\n",
      "  batch 4000 loss: 0.03426320706431925\n",
      "  batch 4500 loss: 0.013091603777037822\n",
      "  batch 5000 loss: 0.014544904794960978\n",
      "  batch 5500 loss: 0.007532563275282584\n",
      "  batch 6000 loss: 0.003962306707226062\n",
      "  batch 6500 loss: 0.010159967195232607\n",
      "  batch 7000 loss: 0.00850462803931094\n",
      "  batch 7500 loss: 0.005049008347985531\n",
      "  batch 8000 loss: 0.01409825330347045\n",
      "  batch 8500 loss: 0.03283588308593923\n",
      "  batch 9000 loss: 0.008625586027032955\n",
      "  batch 9500 loss: 0.044358162183521115\n",
      "  batch 10000 loss: 0.019246406988229978\n",
      "  batch 10500 loss: 0.04447308435436097\n",
      "  batch 11000 loss: 0.017468149539488668\n",
      "  batch 11500 loss: 0.014039736429707446\n",
      "  batch 12000 loss: 0.005048228322324715\n",
      "  batch 12500 loss: 0.0019887475354579075\n",
      "  batch 13000 loss: 0.018819480950194742\n",
      "  batch 13500 loss: 0.02837347651285283\n",
      "  batch 14000 loss: 0.018346941698762113\n",
      "  batch 14500 loss: 0.01178988082450325\n",
      "  batch 15000 loss: 0.014165487964889558\n",
      "  batch 15500 loss: 0.009024532247618967\n",
      "  batch 16000 loss: 0.005821835678026229\n",
      "  batch 16500 loss: 0.006935290282082146\n",
      "  batch 17000 loss: 0.01696255791871924\n",
      "  batch 17500 loss: 0.0031060615752813055\n",
      "  batch 18000 loss: 0.022328429644964613\n",
      "  batch 18500 loss: 0.0036248950816834017\n",
      "  batch 19000 loss: 0.002145868680608487\n",
      "  batch 19500 loss: 0.008125110963987232\n",
      "  batch 20000 loss: 0.008210664658581756\n",
      "  batch 20500 loss: 0.005251976835596935\n",
      "  batch 21000 loss: 0.017084459993563186\n",
      "  batch 21500 loss: 0.008804604894329798\n",
      "  batch 22000 loss: 0.025767520359370712\n",
      "  batch 22500 loss: 0.023509396689554207\n",
      "  batch 23000 loss: 0.008315424218375651\n",
      "  batch 23500 loss: 0.010489229173502984\n",
      "  batch 24000 loss: 0.010794070397681644\n",
      "  batch 24500 loss: 0.013379846969372559\n",
      "  batch 25000 loss: 0.014053177010516543\n",
      "  batch 25500 loss: 0.016297133776562632\n",
      "  batch 26000 loss: 0.02509097480546733\n",
      "  batch 26500 loss: 0.023332454617151412\n",
      "  batch 27000 loss: 0.018583167397392512\n",
      "LOSS train 0.018583167397392512 valid 0.01051340903021884\n",
      "EPOCH 7:\n",
      "  batch 500 loss: 0.010649555106499115\n",
      "  batch 1000 loss: 0.010044449098023127\n",
      "  batch 1500 loss: 0.0022997563234433615\n",
      "  batch 2000 loss: 0.004327476000236689\n",
      "  batch 2500 loss: 0.012200987792445698\n",
      "  batch 3000 loss: 0.010977827688216002\n",
      "  batch 3500 loss: 0.013554329595293431\n",
      "  batch 4000 loss: 0.010640844801701814\n",
      "  batch 4500 loss: 0.005138635215913247\n",
      "  batch 5000 loss: 0.009170941867212136\n",
      "  batch 5500 loss: 0.005187015483676134\n",
      "  batch 6000 loss: 0.04068964147041585\n",
      "  batch 6500 loss: 0.0353191896445209\n",
      "  batch 7000 loss: 0.032600426895804745\n",
      "  batch 7500 loss: 0.009428905135988206\n",
      "  batch 8000 loss: 0.019906846506906224\n",
      "  batch 8500 loss: 0.01788404870002683\n",
      "  batch 9000 loss: 0.009906706664325306\n",
      "  batch 9500 loss: 0.007095412166400255\n",
      "  batch 10000 loss: 0.009931294764679596\n",
      "  batch 10500 loss: 0.002448702730250183\n",
      "  batch 11000 loss: 0.007049847752143897\n",
      "  batch 11500 loss: 0.0026096188161593725\n",
      "  batch 12000 loss: 0.0202302319841572\n",
      "  batch 12500 loss: 0.010868863581116269\n",
      "  batch 13000 loss: 0.00937652575778884\n",
      "  batch 13500 loss: 0.006221293249777904\n",
      "  batch 14000 loss: 0.0021196996868866795\n",
      "  batch 14500 loss: 0.016513137951283004\n",
      "  batch 15000 loss: 0.023519541374123872\n",
      "  batch 15500 loss: 0.004591549122360068\n",
      "  batch 16000 loss: 0.00867939620588967\n",
      "  batch 16500 loss: 0.010924134293993063\n",
      "  batch 17000 loss: 0.028925496438297393\n",
      "  batch 17500 loss: 0.003523552731706879\n",
      "  batch 18000 loss: 0.006969431294162358\n",
      "  batch 18500 loss: 0.02182277551599281\n",
      "  batch 19000 loss: 0.012403432159380098\n",
      "  batch 19500 loss: 0.003267640543130728\n",
      "  batch 20000 loss: 0.02768776705597494\n",
      "  batch 20500 loss: 0.0065873486085685756\n",
      "  batch 21000 loss: 0.004018476089752415\n",
      "  batch 21500 loss: 0.029147080118047185\n",
      "  batch 22000 loss: 0.003411015771681292\n",
      "  batch 22500 loss: 0.00964495406071784\n",
      "  batch 23000 loss: 0.009151984289594193\n",
      "  batch 23500 loss: 0.004163068247450912\n",
      "  batch 24000 loss: 0.008041374316032994\n",
      "  batch 24500 loss: 0.005129664038324844\n",
      "  batch 25000 loss: 0.014823718683471768\n",
      "  batch 25500 loss: 0.013872093897010242\n",
      "  batch 26000 loss: 0.004981975113654105\n",
      "  batch 26500 loss: 0.005560771016351758\n",
      "  batch 27000 loss: 0.0014625568299501488\n",
      "LOSS train 0.0014625568299501488 valid 0.055396695148366544\n",
      "EPOCH 8:\n",
      "  batch 500 loss: 0.015028200273081516\n",
      "  batch 1000 loss: 0.01046206750879442\n",
      "  batch 1500 loss: 0.007118951029428057\n",
      "  batch 2000 loss: 0.008044473569952028\n",
      "  batch 2500 loss: 0.011037449153446644\n",
      "  batch 3000 loss: 0.01722617493656572\n",
      "  batch 3500 loss: 0.01717010258317391\n",
      "  batch 4000 loss: 0.008481707277688244\n",
      "  batch 4500 loss: 0.003948471960974615\n",
      "  batch 5000 loss: 0.0010800162824929735\n",
      "  batch 5500 loss: 0.0017235515664052556\n",
      "  batch 6000 loss: 0.012080964079464941\n",
      "  batch 6500 loss: 0.015159710662579642\n",
      "  batch 7000 loss: 0.006847482336294988\n",
      "  batch 7500 loss: 0.001443674224471728\n",
      "  batch 8000 loss: 0.013298675739888964\n",
      "  batch 8500 loss: 0.0057721478444608355\n",
      "  batch 9000 loss: 0.011394805589891454\n",
      "  batch 9500 loss: 0.003827444603029324\n",
      "  batch 10000 loss: 0.008993938716469568\n",
      "  batch 10500 loss: 0.002708715328783647\n",
      "  batch 11000 loss: 0.0010003039068914035\n",
      "  batch 11500 loss: 0.003002782002479961\n",
      "  batch 12000 loss: 0.028197362455370762\n",
      "  batch 12500 loss: 0.010047432039416005\n",
      "  batch 13000 loss: 0.008750904955011017\n",
      "  batch 13500 loss: 0.0032534036250709093\n",
      "  batch 14000 loss: 0.025634657983610063\n",
      "  batch 14500 loss: 0.012843683969964487\n",
      "  batch 15000 loss: 0.026612110232027857\n",
      "  batch 15500 loss: 0.01705060164571062\n",
      "  batch 16000 loss: 0.004462922264919509\n",
      "  batch 16500 loss: 0.016111885722874048\n",
      "  batch 17000 loss: 0.005015591095525078\n",
      "  batch 17500 loss: 0.003952829692860146\n",
      "  batch 18000 loss: 0.016994707527064588\n",
      "  batch 18500 loss: 0.004431699589079155\n",
      "  batch 19000 loss: 0.001779217060459473\n",
      "  batch 19500 loss: 0.00954278036218541\n",
      "  batch 20000 loss: 0.0035346040062533605\n",
      "  batch 20500 loss: 0.009106791658163534\n",
      "  batch 21000 loss: 0.011166289322049603\n",
      "  batch 21500 loss: 0.001061278990528919\n",
      "  batch 22000 loss: 0.013015289909507573\n",
      "  batch 22500 loss: 0.013211149785326597\n",
      "  batch 23000 loss: 0.014901231140658158\n",
      "  batch 23500 loss: 0.004886585735470781\n",
      "  batch 24000 loss: 0.0037759025256985518\n",
      "  batch 24500 loss: 0.001200287751152583\n",
      "  batch 25000 loss: 0.002663456944835559\n",
      "  batch 25500 loss: 0.037250594671038056\n",
      "  batch 26000 loss: 0.009030435819038238\n",
      "  batch 26500 loss: 0.004922241846531751\n",
      "  batch 27000 loss: 0.003865359258284819\n",
      "LOSS train 0.003865359258284819 valid 0.003997674319233969\n",
      "EPOCH 9:\n",
      "  batch 500 loss: 0.010897219330799046\n",
      "  batch 1000 loss: 0.0008953324178246796\n",
      "  batch 1500 loss: 0.004910609936593467\n",
      "  batch 2000 loss: 0.006022994425047997\n",
      "  batch 2500 loss: 0.007959413549922797\n",
      "  batch 3000 loss: 0.003871756375283592\n",
      "  batch 3500 loss: 0.0020009003616323893\n",
      "  batch 4000 loss: 0.0066163161696696305\n",
      "  batch 4500 loss: 0.0006893375110336599\n",
      "  batch 5000 loss: 0.004604431859975755\n",
      "  batch 5500 loss: 0.008733913673989405\n",
      "  batch 6000 loss: 0.006732310151773767\n",
      "  batch 6500 loss: 0.010854343579911703\n",
      "  batch 7000 loss: 0.00550444591486151\n",
      "  batch 7500 loss: 0.028888842536039905\n",
      "  batch 8000 loss: 0.007703494652529514\n",
      "  batch 8500 loss: 0.009218806211782851\n",
      "  batch 9000 loss: 0.0022834413448428598\n",
      "  batch 9500 loss: 0.0019617488488137768\n",
      "  batch 10000 loss: 0.014097807016673297\n",
      "  batch 10500 loss: 0.018820123110569104\n",
      "  batch 11000 loss: 0.0009895379431669547\n",
      "  batch 11500 loss: 0.016495504462932273\n",
      "  batch 12000 loss: 0.001454767631213734\n",
      "  batch 12500 loss: 0.0009086406242826648\n",
      "  batch 13000 loss: 0.006200995515270318\n",
      "  batch 13500 loss: 0.0015265602223967854\n",
      "  batch 14000 loss: 0.0013923171705443806\n",
      "  batch 14500 loss: 0.02576934208425465\n",
      "  batch 15000 loss: 0.012921616694514082\n",
      "  batch 15500 loss: 0.007869964976683803\n",
      "  batch 16000 loss: 0.005238384675681686\n",
      "  batch 16500 loss: 0.002542355691986625\n",
      "  batch 17000 loss: 0.021352432980441576\n",
      "  batch 17500 loss: 0.013776391292618737\n",
      "  batch 18000 loss: 0.0017940573581320507\n",
      "  batch 18500 loss: 0.0036068732812076475\n",
      "  batch 19000 loss: 0.0037738407186276497\n",
      "  batch 19500 loss: 0.010355949008345328\n",
      "  batch 20000 loss: 0.002825996235446446\n",
      "  batch 20500 loss: 0.0018373465271345282\n",
      "  batch 21000 loss: 0.011578038866519933\n",
      "  batch 21500 loss: 0.0013325991103786557\n",
      "  batch 22000 loss: 0.019192096809933726\n",
      "  batch 22500 loss: 0.001501811701782941\n",
      "  batch 23000 loss: 0.00935616168171763\n",
      "  batch 23500 loss: 0.01175678002217551\n",
      "  batch 24000 loss: 0.009262291888812552\n",
      "  batch 24500 loss: 0.0008666632789240474\n",
      "  batch 25000 loss: 0.01689242907855231\n",
      "  batch 25500 loss: 0.011551877410285887\n",
      "  batch 26000 loss: 0.045721121105676006\n",
      "  batch 26500 loss: 0.00395588855069418\n",
      "  batch 27000 loss: 0.02370959724553048\n",
      "LOSS train 0.02370959724553048 valid 0.008061507009347522\n",
      "EPOCH 10:\n",
      "  batch 500 loss: 0.0010722078880006868\n",
      "  batch 1000 loss: 0.0031768423519244566\n",
      "  batch 1500 loss: 0.005807162270582289\n",
      "  batch 2000 loss: 0.0008525012365872655\n",
      "  batch 2500 loss: 0.005477312615878098\n",
      "  batch 3000 loss: 0.0013034875277393673\n",
      "  batch 3500 loss: 0.002518983302612991\n",
      "  batch 4000 loss: 0.0008935032970973111\n",
      "  batch 4500 loss: 0.006215684233930272\n",
      "  batch 5000 loss: 0.018513965174437114\n",
      "  batch 5500 loss: 0.008693928192152032\n",
      "  batch 6000 loss: 0.0011416830759537078\n",
      "  batch 6500 loss: 0.014439111905096908\n",
      "  batch 7000 loss: 0.0028529581145144006\n",
      "  batch 7500 loss: 0.007879687911220967\n",
      "  batch 8000 loss: 0.0047739469271197875\n",
      "  batch 8500 loss: 0.013982117467674523\n",
      "  batch 9000 loss: 0.008624816687906992\n",
      "  batch 9500 loss: 0.0078368369455164\n",
      "  batch 10000 loss: 0.003493740032186018\n",
      "  batch 10500 loss: 0.0023556956972722036\n",
      "  batch 11000 loss: 0.00397786206839951\n",
      "  batch 11500 loss: 0.02843007754489129\n",
      "  batch 12000 loss: 0.028840650277127535\n",
      "  batch 12500 loss: 0.009266050194928845\n",
      "  batch 13000 loss: 0.007766034073238632\n",
      "  batch 13500 loss: 0.006432188375235548\n",
      "  batch 14000 loss: 0.00818311201645833\n",
      "  batch 14500 loss: 0.011743281352673737\n",
      "  batch 15000 loss: 0.0007845524659129879\n",
      "  batch 15500 loss: 0.00123840407495425\n",
      "  batch 16000 loss: 0.007133635359251972\n",
      "  batch 16500 loss: 0.010324990903757566\n",
      "  batch 17000 loss: 0.010827584054172277\n",
      "  batch 17500 loss: 0.004983152940401307\n",
      "  batch 18000 loss: 0.002041082223743917\n",
      "  batch 18500 loss: 0.0105868857614951\n",
      "  batch 19000 loss: 0.004225080886265917\n",
      "  batch 19500 loss: 0.0036804794699885584\n",
      "  batch 20000 loss: 0.002851809550627532\n",
      "  batch 20500 loss: 0.0018977402244495032\n",
      "  batch 21000 loss: 0.002111328247016381\n",
      "  batch 21500 loss: 0.006306539048159376\n",
      "  batch 22000 loss: 0.005088356878607662\n",
      "  batch 22500 loss: 0.007289304507569746\n",
      "  batch 23000 loss: 0.010722052543257416\n",
      "  batch 23500 loss: 0.005795967408697433\n",
      "  batch 24000 loss: 0.0041711547186123925\n",
      "  batch 24500 loss: 0.0008416776220073849\n",
      "  batch 25000 loss: 0.015351879114234312\n",
      "  batch 25500 loss: 0.012108656053371775\n",
      "  batch 26000 loss: 0.0093641331735232\n",
      "  batch 26500 loss: 0.002538505879037512\n",
      "  batch 27000 loss: 0.0009924542146850079\n",
      "LOSS train 0.0009924542146850079 valid 0.02287076867495269\n",
      "EPOCH 11:\n",
      "  batch 500 loss: 0.024418931152545606\n",
      "  batch 1000 loss: 0.019112418281366336\n",
      "  batch 1500 loss: 0.0035418883691676123\n",
      "  batch 2000 loss: 0.006842332376607416\n",
      "  batch 2500 loss: 0.009592435586608822\n",
      "  batch 3000 loss: 0.005917456980767771\n",
      "  batch 3500 loss: 0.004343904911748122\n",
      "  batch 4000 loss: 0.005292636493582229\n",
      "  batch 4500 loss: 0.031138862917213874\n",
      "  batch 5000 loss: 0.0008343527056477207\n",
      "  batch 5500 loss: 0.0011634497867835378\n",
      "  batch 6000 loss: 0.0028183612854419415\n",
      "  batch 6500 loss: 0.0013746642318965385\n",
      "  batch 7000 loss: 0.006060472767633527\n",
      "  batch 7500 loss: 0.010159026313375116\n",
      "  batch 8000 loss: 0.001637775534137159\n",
      "  batch 8500 loss: 0.010567496003699023\n",
      "  batch 9000 loss: 0.001836397914537983\n",
      "  batch 9500 loss: 0.010111983608701717\n",
      "  batch 10000 loss: 0.012255701379127019\n",
      "  batch 10500 loss: 0.0041105590692487115\n",
      "  batch 11000 loss: 0.008030015875216186\n",
      "  batch 11500 loss: 0.018441974423406927\n",
      "  batch 12000 loss: 0.014612682677215794\n",
      "  batch 12500 loss: 0.010027624023168872\n",
      "  batch 13000 loss: 0.0013685944472260196\n",
      "  batch 13500 loss: 0.0029070441954152527\n",
      "  batch 14000 loss: 0.000605049627915701\n",
      "  batch 14500 loss: 0.0010199363498029222\n",
      "  batch 15000 loss: 0.006056091666215331\n",
      "  batch 15500 loss: 0.023608080113343383\n",
      "  batch 16000 loss: 0.00750157438929035\n",
      "  batch 16500 loss: 0.0048596765292467725\n",
      "  batch 17000 loss: 0.0033806762860152943\n",
      "  batch 17500 loss: 0.00634862660512259\n",
      "  batch 18000 loss: 0.009978057912956131\n",
      "  batch 18500 loss: 0.004792336649210497\n",
      "  batch 19000 loss: 0.0010239111561668075\n",
      "  batch 19500 loss: 0.010050314355133655\n",
      "  batch 20000 loss: 0.019158160527855253\n",
      "  batch 20500 loss: 0.006525419511557459\n",
      "  batch 21000 loss: 0.024369058018550792\n",
      "  batch 21500 loss: 0.020424417342951674\n",
      "  batch 22000 loss: 0.011696713993876055\n",
      "  batch 22500 loss: 0.004805400240389193\n",
      "  batch 23000 loss: 0.0012257759660183325\n",
      "  batch 23500 loss: 0.0005771586372109532\n",
      "  batch 24000 loss: 0.0009708890888283293\n",
      "  batch 24500 loss: 0.022907562154649272\n",
      "  batch 25000 loss: 0.0036674821837225373\n",
      "  batch 25500 loss: 0.010126872773339795\n",
      "  batch 26000 loss: 0.003891088209963048\n",
      "  batch 26500 loss: 0.005434479718591565\n",
      "  batch 27000 loss: 0.012198964898799186\n",
      "LOSS train 0.012198964898799186 valid 0.00384183806840324\n",
      "EPOCH 12:\n",
      "  batch 500 loss: 0.004413934595870608\n",
      "  batch 1000 loss: 0.002811214617318022\n",
      "  batch 1500 loss: 0.000775807289489034\n",
      "  batch 2000 loss: 0.0099158818592288\n",
      "  batch 2500 loss: 0.0005797993390938671\n",
      "  batch 3000 loss: 0.007795857195278163\n",
      "  batch 3500 loss: 0.0007469546707287194\n",
      "  batch 4000 loss: 0.00035829865733129297\n",
      "  batch 4500 loss: 0.0004414566577440908\n",
      "  batch 5000 loss: 0.0007485274224661716\n",
      "  batch 5500 loss: 0.0006262977619042545\n",
      "  batch 6000 loss: 0.0025620634650871674\n",
      "  batch 6500 loss: 0.0005692170225583908\n",
      "  batch 7000 loss: 0.00043647459216525065\n",
      "  batch 7500 loss: 0.0005882674861788466\n",
      "  batch 8000 loss: 0.008101934558438717\n",
      "  batch 8500 loss: 0.029276392380909535\n",
      "  batch 9000 loss: 0.01710044539380789\n",
      "  batch 9500 loss: 0.014931894941113381\n",
      "  batch 10000 loss: 0.00830680518003702\n",
      "  batch 10500 loss: 0.001984997880732838\n",
      "  batch 11000 loss: 0.0024531416916540073\n",
      "  batch 11500 loss: 0.0082471685654406\n",
      "  batch 12000 loss: 0.003750785386761674\n",
      "  batch 12500 loss: 0.00519015867928162\n",
      "  batch 13000 loss: 0.0080555674349959\n",
      "  batch 13500 loss: 0.01092034457602772\n",
      "  batch 14000 loss: 0.0032676156250150577\n",
      "  batch 14500 loss: 0.009656548439395621\n",
      "  batch 15000 loss: 0.0022948372445425634\n",
      "  batch 15500 loss: 0.0014083053192275692\n",
      "  batch 16000 loss: 0.012974627365571752\n",
      "  batch 16500 loss: 0.0027014824922589613\n",
      "  batch 17000 loss: 0.0031882130810450314\n",
      "  batch 17500 loss: 0.00046968622299927175\n",
      "  batch 18000 loss: 0.004756074709135181\n",
      "  batch 18500 loss: 0.001442796436116115\n",
      "  batch 19000 loss: 0.00811124167946979\n",
      "  batch 19500 loss: 0.003034616353472394\n",
      "  batch 20000 loss: 0.006228473082427207\n",
      "  batch 20500 loss: 0.011260453605002428\n",
      "  batch 21000 loss: 0.0010049250212584475\n",
      "  batch 21500 loss: 0.0007207251309119798\n",
      "  batch 22000 loss: 0.006935728160983047\n",
      "  batch 22500 loss: 0.002861938760710128\n",
      "  batch 23000 loss: 0.0006555124024045007\n",
      "  batch 23500 loss: 0.0007344404464056104\n",
      "  batch 24000 loss: 0.0006840384396261818\n",
      "  batch 24500 loss: 0.001403675153806798\n",
      "  batch 25000 loss: 0.0009477284469813939\n",
      "  batch 25500 loss: 0.0006736382800827059\n",
      "  batch 26000 loss: 0.0016263447229869605\n",
      "  batch 26500 loss: 0.0031135394891833633\n",
      "  batch 27000 loss: 0.01451878811620865\n",
      "LOSS train 0.01451878811620865 valid 0.004314002785318359\n",
      "EPOCH 13:\n",
      "  batch 500 loss: 0.038940250692910294\n",
      "  batch 1000 loss: 0.00488708235015751\n",
      "  batch 1500 loss: 0.0072711529300810195\n",
      "  batch 2000 loss: 0.0051547024381136805\n",
      "  batch 2500 loss: 0.00043465526801485764\n",
      "  batch 3000 loss: 0.004665691430317537\n",
      "  batch 3500 loss: 0.005116353627760393\n",
      "  batch 4000 loss: 0.001263310501431974\n",
      "  batch 4500 loss: 0.009575833169833618\n",
      "  batch 5000 loss: 0.002849078765539044\n",
      "  batch 5500 loss: 0.0004914532137197902\n",
      "  batch 6000 loss: 0.0011216397053643164\n",
      "  batch 6500 loss: 0.009187123706355443\n",
      "  batch 7000 loss: 0.004502907553438995\n",
      "  batch 7500 loss: 0.0007796327323356281\n",
      "  batch 8000 loss: 0.000737009399253779\n",
      "  batch 8500 loss: 0.016868422309144412\n",
      "  batch 9000 loss: 0.0009987826069316342\n",
      "  batch 9500 loss: 0.013008408202982576\n",
      "  batch 10000 loss: 0.0026265220233345553\n",
      "  batch 10500 loss: 0.012100773745850764\n",
      "  batch 11000 loss: 0.003538233083506302\n",
      "  batch 11500 loss: 0.0074627718997746124\n",
      "  batch 12000 loss: 0.001556256614316382\n",
      "  batch 12500 loss: 0.0009858636736785051\n",
      "  batch 13000 loss: 0.0012178380017786132\n",
      "  batch 13500 loss: 0.0015619001547312976\n",
      "  batch 14000 loss: 0.0051673006384575755\n",
      "  batch 14500 loss: 0.005200956508388881\n",
      "  batch 15000 loss: 0.039587634288373405\n",
      "  batch 15500 loss: 0.0130920670695329\n",
      "  batch 16000 loss: 0.0006454577034174349\n",
      "  batch 16500 loss: 0.0053078749742796845\n",
      "  batch 17000 loss: 0.0020484765648969053\n",
      "  batch 17500 loss: 0.002365556672695906\n",
      "  batch 18000 loss: 0.0046147851680198105\n",
      "  batch 18500 loss: 0.0013511092649783265\n",
      "  batch 19000 loss: 0.001035424389284298\n",
      "  batch 19500 loss: 0.005807687645834985\n",
      "  batch 20000 loss: 0.007156113227079036\n",
      "  batch 20500 loss: 0.012274002489997032\n",
      "  batch 21000 loss: 0.003002563419249576\n",
      "  batch 21500 loss: 0.0010247146308777282\n",
      "  batch 22000 loss: 0.007024291811287977\n",
      "  batch 22500 loss: 0.0005001148313077622\n",
      "  batch 23000 loss: 0.002611952052771688\n",
      "  batch 23500 loss: 0.006303283590441264\n",
      "  batch 24000 loss: 0.0064964874014614435\n",
      "  batch 24500 loss: 0.0006354603001472299\n",
      "  batch 25000 loss: 0.007827963389402556\n",
      "  batch 25500 loss: 0.006295559459439619\n",
      "  batch 26000 loss: 0.02697440292451417\n",
      "  batch 26500 loss: 0.008018262924126458\n",
      "  batch 27000 loss: 0.0022850218104110416\n",
      "LOSS train 0.0022850218104110416 valid 0.0036095523190534545\n",
      "EPOCH 14:\n",
      "  batch 500 loss: 0.015240527118421056\n",
      "  batch 1000 loss: 0.0004769512532128282\n",
      "  batch 1500 loss: 0.0004640019000429021\n",
      "  batch 2000 loss: 0.0022856580510804817\n",
      "  batch 2500 loss: 0.0024437107419924653\n",
      "  batch 3000 loss: 0.000490942952501932\n",
      "  batch 3500 loss: 0.0015559113893804195\n",
      "  batch 4000 loss: 0.0068014343853309\n",
      "  batch 4500 loss: 0.0004729638608502924\n",
      "  batch 5000 loss: 0.015641017550068886\n",
      "  batch 5500 loss: 0.0015078729198877384\n",
      "  batch 6000 loss: 0.007417667482174647\n",
      "  batch 6500 loss: 0.001731885213670285\n",
      "  batch 7000 loss: 0.0020865355093096128\n",
      "  batch 7500 loss: 0.004369146490194812\n",
      "  batch 8000 loss: 0.011206253263368698\n",
      "  batch 8500 loss: 0.0015229100836142279\n",
      "  batch 9000 loss: 0.0006351555110790947\n",
      "  batch 9500 loss: 0.00596147504876755\n",
      "  batch 10000 loss: 0.01667408831386981\n",
      "  batch 10500 loss: 0.0038176591761950574\n",
      "  batch 11000 loss: 0.005016452309286585\n",
      "  batch 11500 loss: 0.001992353396501457\n",
      "  batch 12000 loss: 0.00718044761093308\n",
      "  batch 12500 loss: 0.006649081058691877\n",
      "  batch 13000 loss: 0.0062368144156240745\n",
      "  batch 13500 loss: 0.0004241428358607955\n",
      "  batch 14000 loss: 0.0020015737625885812\n",
      "  batch 14500 loss: 0.019143255396547484\n",
      "  batch 15000 loss: 0.005035414424600183\n",
      "  batch 15500 loss: 0.0009394646589367923\n",
      "  batch 16000 loss: 0.0005603061865987727\n",
      "  batch 16500 loss: 0.0006685356417905766\n",
      "  batch 17000 loss: 0.02366843437780537\n",
      "  batch 17500 loss: 0.027961784535566053\n",
      "  batch 18000 loss: 0.007326497474066354\n",
      "  batch 18500 loss: 0.00656016432521611\n",
      "  batch 19000 loss: 0.002023536298774136\n",
      "  batch 19500 loss: 0.0007036341681265128\n",
      "  batch 20000 loss: 0.0035799778107688312\n",
      "  batch 20500 loss: 0.0026864059217261128\n",
      "  batch 21000 loss: 0.0017497871294750204\n",
      "  batch 21500 loss: 0.0016590351524116934\n",
      "  batch 22000 loss: 0.0006105751981453053\n",
      "  batch 22500 loss: 0.01037916416189442\n",
      "  batch 23000 loss: 0.02146111265767345\n",
      "  batch 23500 loss: 0.018187402254599354\n",
      "  batch 24000 loss: 0.001370677198085673\n",
      "  batch 24500 loss: 0.005338267420148394\n",
      "  batch 25000 loss: 0.004616964195281468\n",
      "  batch 25500 loss: 0.000999986797857801\n",
      "  batch 26000 loss: 0.0009645443132993669\n",
      "  batch 26500 loss: 0.00048277058054572563\n",
      "  batch 27000 loss: 0.004685577573668891\n",
      "LOSS train 0.004685577573668891 valid 0.004275903111515943\n",
      "EPOCH 15:\n",
      "  batch 500 loss: 0.009802258598541442\n",
      "  batch 1000 loss: 0.0029510953267355405\n",
      "  batch 1500 loss: 0.003405517394928129\n",
      "  batch 2000 loss: 0.006572300376736589\n",
      "  batch 2500 loss: 0.01196672722188728\n",
      "  batch 3000 loss: 0.0029006649972087004\n",
      "  batch 3500 loss: 0.0020953389760233563\n",
      "  batch 4000 loss: 0.0027486688206314583\n",
      "  batch 4500 loss: 0.0020526501273063573\n",
      "  batch 5000 loss: 0.019082298105974692\n",
      "  batch 5500 loss: 0.005537019186180675\n",
      "  batch 6000 loss: 0.001383674119261663\n",
      "  batch 6500 loss: 0.0055817474938292195\n",
      "  batch 7000 loss: 0.0019201489556235423\n",
      "  batch 7500 loss: 0.0012296760345919644\n",
      "  batch 8000 loss: 0.0026311938769105867\n",
      "  batch 8500 loss: 0.0005418477788074938\n",
      "  batch 9000 loss: 0.003933391854577671\n",
      "  batch 9500 loss: 0.003580196691197074\n",
      "  batch 10000 loss: 0.0010149427000826349\n",
      "  batch 10500 loss: 0.0016917665908021816\n",
      "  batch 11000 loss: 0.003563403974628045\n",
      "  batch 11500 loss: 0.0027707599590237135\n",
      "  batch 12000 loss: 0.00366310032451689\n",
      "  batch 12500 loss: 0.00048054194739579616\n",
      "  batch 13000 loss: 0.00036689514887483553\n",
      "  batch 13500 loss: 0.0005754214715915396\n",
      "  batch 14000 loss: 0.0039238829653437364\n",
      "  batch 14500 loss: 0.005679789313417491\n",
      "  batch 15000 loss: 0.00029896681047368644\n",
      "  batch 15500 loss: 0.003478946030358166\n",
      "  batch 16000 loss: 0.021851175530085913\n",
      "  batch 16500 loss: 0.021259027829396687\n",
      "  batch 17000 loss: 0.0009796832724280548\n",
      "  batch 17500 loss: 0.0032857461635823065\n",
      "  batch 18000 loss: 0.0011881719298998624\n",
      "  batch 18500 loss: 0.0018475665597737568\n",
      "  batch 19000 loss: 0.0005646685589687444\n",
      "  batch 19500 loss: 0.007862934472549644\n",
      "  batch 20000 loss: 0.0005888760127061125\n",
      "  batch 20500 loss: 0.00162644142933598\n",
      "  batch 21000 loss: 0.0005150793989733131\n",
      "  batch 21500 loss: 0.0004168061367098694\n",
      "  batch 22000 loss: 0.0016990254489284737\n",
      "  batch 22500 loss: 0.0005417032292870445\n",
      "  batch 23000 loss: 0.0011540243483994814\n",
      "  batch 23500 loss: 0.00132843823683082\n",
      "  batch 24000 loss: 0.0008500519878888753\n",
      "  batch 24500 loss: 0.00017716188361345075\n",
      "  batch 25000 loss: 0.032159922495607306\n",
      "  batch 25500 loss: 0.00859633250362264\n",
      "  batch 26000 loss: 0.006009180746755063\n",
      "  batch 26500 loss: 0.0004358205713948493\n",
      "  batch 27000 loss: 0.002020669556173363\n",
      "LOSS train 0.002020669556173363 valid 0.0027273995096956144\n",
      "EPOCH 16:\n",
      "  batch 500 loss: 0.005453061701777777\n",
      "  batch 1000 loss: 0.002767222866547893\n",
      "  batch 1500 loss: 0.006973475847602827\n",
      "  batch 2000 loss: 0.0007231221908897716\n",
      "  batch 2500 loss: 0.0034929960040988315\n",
      "  batch 3000 loss: 0.007186032635146447\n",
      "  batch 3500 loss: 0.0017773190272666747\n",
      "  batch 4000 loss: 0.0005539023029759029\n",
      "  batch 4500 loss: 0.00050197638978646\n",
      "  batch 5000 loss: 0.0011253629169462407\n",
      "  batch 5500 loss: 0.0004480191275568899\n",
      "  batch 6000 loss: 0.007027602094443964\n",
      "  batch 6500 loss: 0.004274040340170224\n",
      "  batch 7000 loss: 0.0027802090551148667\n",
      "  batch 7500 loss: 0.00031289292941861645\n",
      "  batch 8000 loss: 0.011443424545643613\n",
      "  batch 8500 loss: 0.004423222066036189\n",
      "  batch 9000 loss: 0.005372998297082759\n",
      "  batch 9500 loss: 0.009821375477744417\n",
      "  batch 10000 loss: 0.0011103266538843215\n",
      "  batch 10500 loss: 0.0006146407947735994\n",
      "  batch 11000 loss: 0.0027074824030264787\n",
      "  batch 11500 loss: 0.0010878629120546143\n",
      "  batch 12000 loss: 0.0029552313110723176\n",
      "  batch 12500 loss: 0.0017836963471212534\n",
      "  batch 13000 loss: 0.006682135683143176\n",
      "  batch 13500 loss: 0.007904007328911682\n",
      "  batch 14000 loss: 0.0006456918784731051\n",
      "  batch 14500 loss: 0.005649915971630275\n",
      "  batch 15000 loss: 0.0004869831047101805\n",
      "  batch 15500 loss: 0.0007128521317704788\n",
      "  batch 16000 loss: 0.003822678060985556\n",
      "  batch 16500 loss: 0.013933914066008078\n",
      "  batch 17000 loss: 0.00713216692010116\n",
      "  batch 17500 loss: 0.010239709492501092\n",
      "  batch 18000 loss: 0.008956908255487821\n",
      "  batch 18500 loss: 0.0003961307397938718\n",
      "  batch 19000 loss: 0.0025043172319072367\n",
      "  batch 19500 loss: 0.0011714837664695815\n",
      "  batch 20000 loss: 0.0030006862946124284\n",
      "  batch 20500 loss: 0.0007680109761283909\n",
      "  batch 21000 loss: 0.0007767595774951061\n",
      "  batch 21500 loss: 0.00803401788384462\n",
      "  batch 22000 loss: 0.0015584503477386847\n",
      "  batch 22500 loss: 0.002852383826985939\n",
      "  batch 23000 loss: 0.002726803853133383\n",
      "  batch 23500 loss: 0.00039484249523459526\n",
      "  batch 24000 loss: 0.00025202709400920753\n",
      "  batch 24500 loss: 0.0018145607654096202\n",
      "  batch 25000 loss: 0.00029652530817280364\n",
      "  batch 25500 loss: 0.014759903989530122\n",
      "  batch 26000 loss: 0.0016935178982326598\n",
      "  batch 26500 loss: 0.0008040664736783896\n",
      "  batch 27000 loss: 0.000330139281948874\n",
      "LOSS train 0.000330139281948874 valid 0.002607475131024042\n",
      "EPOCH 17:\n",
      "  batch 500 loss: 0.0005320691450813762\n",
      "  batch 1000 loss: 0.0005128926264260158\n",
      "  batch 1500 loss: 0.0005042060162968802\n",
      "  batch 2000 loss: 0.008132203800241567\n",
      "  batch 2500 loss: 0.00041714438402948415\n",
      "  batch 3000 loss: 0.0010897815349911256\n",
      "  batch 3500 loss: 0.03323683070980767\n",
      "  batch 4000 loss: 0.005321819446677985\n",
      "  batch 4500 loss: 0.001177788684110844\n",
      "  batch 5000 loss: 0.0006623125669930267\n",
      "  batch 5500 loss: 0.01375510095922505\n",
      "  batch 6000 loss: 0.0035661837223080256\n",
      "  batch 6500 loss: 0.0010196548897750013\n",
      "  batch 7000 loss: 0.0026315784145313436\n",
      "  batch 7500 loss: 0.0007458933324209375\n",
      "  batch 8000 loss: 0.0031501475904644567\n",
      "  batch 8500 loss: 0.0004796817395316708\n",
      "  batch 9000 loss: 0.004985727858833773\n",
      "  batch 9500 loss: 0.00039526951395891174\n",
      "  batch 10000 loss: 0.0011946431355675457\n",
      "  batch 10500 loss: 0.0006140548795726133\n",
      "  batch 11000 loss: 0.0005655155696971797\n",
      "  batch 11500 loss: 0.000920894291884423\n",
      "  batch 12000 loss: 0.001956592833368568\n",
      "  batch 12500 loss: 0.004949585234580692\n",
      "  batch 13000 loss: 0.005557672792385873\n",
      "  batch 13500 loss: 0.004808391491908687\n",
      "  batch 14000 loss: 0.00045793691564075087\n",
      "  batch 14500 loss: 0.0007004285852794006\n",
      "  batch 15000 loss: 0.0021483406989918023\n",
      "  batch 15500 loss: 0.006689631960410334\n",
      "  batch 16000 loss: 0.0003230603094202742\n",
      "  batch 16500 loss: 0.0003745651651163548\n",
      "  batch 17000 loss: 0.0007356855320518783\n",
      "  batch 17500 loss: 0.0008058429294663548\n",
      "  batch 18000 loss: 0.009182886847971325\n",
      "  batch 18500 loss: 0.0028206125969634215\n",
      "  batch 19000 loss: 0.0003264548038569153\n",
      "  batch 19500 loss: 0.004145310761164772\n",
      "  batch 20000 loss: 0.005005353221985129\n",
      "  batch 20500 loss: 0.013347302956199179\n",
      "  batch 21000 loss: 0.002026435411037184\n",
      "  batch 21500 loss: 0.0006276123335771117\n",
      "  batch 22000 loss: 0.0014429172037331384\n",
      "  batch 22500 loss: 0.019739338219888443\n",
      "  batch 23000 loss: 0.006891388366000317\n",
      "  batch 23500 loss: 0.015443610155614901\n",
      "  batch 24000 loss: 0.0010542041058778616\n",
      "  batch 24500 loss: 0.006620999391383211\n",
      "  batch 25000 loss: 0.0009696971884685333\n",
      "  batch 25500 loss: 0.00039936104386818625\n",
      "  batch 26000 loss: 0.0005638752750407221\n",
      "  batch 26500 loss: 0.0004787884500159265\n",
      "  batch 27000 loss: 0.002672181313135134\n",
      "LOSS train 0.002672181313135134 valid 0.0023353883406676042\n",
      "EPOCH 18:\n",
      "  batch 500 loss: 0.016380098995643952\n",
      "  batch 1000 loss: 0.01213366766425037\n",
      "  batch 1500 loss: 0.001274987864357552\n",
      "  batch 2000 loss: 0.0017329337231717084\n",
      "  batch 2500 loss: 0.00045296955504221613\n",
      "  batch 3000 loss: 0.0002475217632606963\n",
      "  batch 3500 loss: 0.000695409545792657\n",
      "  batch 4000 loss: 0.00028888067434984066\n",
      "  batch 4500 loss: 0.0007691501332305961\n",
      "  batch 5000 loss: 0.008679266009488923\n",
      "  batch 5500 loss: 0.00034465245718019234\n",
      "  batch 6000 loss: 0.0007048856750110453\n",
      "  batch 6500 loss: 0.000885428755119019\n",
      "  batch 7000 loss: 0.005082452539175129\n",
      "  batch 7500 loss: 0.00027997063930692076\n",
      "  batch 8000 loss: 0.006796156767773852\n",
      "  batch 8500 loss: 0.005396782788253351\n",
      "  batch 9000 loss: 0.004401587501301055\n",
      "  batch 9500 loss: 0.0004657891989481193\n",
      "  batch 10000 loss: 0.006529474342411526\n",
      "  batch 10500 loss: 0.013699379842833195\n",
      "  batch 11000 loss: 0.0015199328314442794\n",
      "  batch 11500 loss: 0.004448489590771438\n",
      "  batch 12000 loss: 0.0036039584796441807\n",
      "  batch 12500 loss: 0.0005921920009632692\n",
      "  batch 13000 loss: 0.0006047163268989096\n",
      "  batch 13500 loss: 0.001885946928366252\n",
      "  batch 14000 loss: 0.014058193793648564\n",
      "  batch 14500 loss: 0.013409867653260299\n",
      "  batch 15000 loss: 0.0004939848456317293\n",
      "  batch 15500 loss: 0.0003520278097299112\n",
      "  batch 16000 loss: 0.0005017853349419177\n",
      "  batch 16500 loss: 0.000685845754109149\n",
      "  batch 17000 loss: 0.0011060579369793224\n",
      "  batch 17500 loss: 0.0002944529016376549\n",
      "  batch 18000 loss: 0.001108580669238396\n",
      "  batch 18500 loss: 0.0009008105734002818\n",
      "  batch 19000 loss: 0.0005560176719948338\n",
      "  batch 19500 loss: 0.006763979860715388\n",
      "  batch 20000 loss: 0.0006645190075781215\n",
      "  batch 20500 loss: 0.0030694852285996924\n",
      "  batch 21000 loss: 0.001347876378891943\n",
      "  batch 21500 loss: 0.00024246476163341057\n",
      "  batch 22000 loss: 0.0005225035204447792\n",
      "  batch 22500 loss: 0.002007065187867454\n",
      "  batch 23000 loss: 0.0006169313700645737\n",
      "  batch 23500 loss: 0.00018381021656743002\n",
      "  batch 24000 loss: 0.000861476891901539\n",
      "  batch 24500 loss: 0.0013563835056097631\n",
      "  batch 25000 loss: 0.0035283318679808565\n",
      "  batch 25500 loss: 0.006693212155330595\n",
      "  batch 26000 loss: 0.0008617057254727385\n",
      "  batch 26500 loss: 0.0005460048739134038\n",
      "  batch 27000 loss: 0.010903688582843376\n",
      "LOSS train 0.010903688582843376 valid 0.004547063815443849\n",
      "EPOCH 19:\n",
      "  batch 500 loss: 0.005937874883867764\n",
      "  batch 1000 loss: 0.0009067912420277438\n",
      "  batch 1500 loss: 0.01956431817208367\n",
      "  batch 2000 loss: 0.04126804268026586\n",
      "  batch 2500 loss: 0.0024108427324720694\n",
      "  batch 3000 loss: 0.005446542013555586\n",
      "  batch 3500 loss: 0.01603008586043781\n",
      "  batch 4000 loss: 0.001306583108328386\n",
      "  batch 4500 loss: 0.0008157165259231043\n",
      "  batch 5000 loss: 0.0023677857256075006\n",
      "  batch 5500 loss: 0.0036386231213859474\n",
      "  batch 6000 loss: 0.0033388782860587726\n",
      "  batch 6500 loss: 0.0010326586023877446\n",
      "  batch 7000 loss: 0.0005801277090901493\n",
      "  batch 7500 loss: 0.022382492721327273\n",
      "  batch 8000 loss: 0.002080016720790809\n",
      "  batch 8500 loss: 0.001022278708892543\n",
      "  batch 9000 loss: 0.002726394757371349\n",
      "  batch 9500 loss: 0.0012965821659405242\n",
      "  batch 10000 loss: 0.000170985943904725\n",
      "  batch 10500 loss: 0.0007362195977316439\n",
      "  batch 11000 loss: 0.000606324034371621\n",
      "  batch 11500 loss: 0.006663894302886216\n",
      "  batch 12000 loss: 0.0003433011505438479\n",
      "  batch 12500 loss: 0.0024948514731945757\n",
      "  batch 13000 loss: 0.0002626634210262786\n",
      "  batch 13500 loss: 0.004336194094585085\n",
      "  batch 14000 loss: 0.0003911610469960252\n",
      "  batch 14500 loss: 0.0011413246157184922\n",
      "  batch 15000 loss: 0.0026531944818011938\n",
      "  batch 15500 loss: 0.00037138894312316496\n",
      "  batch 16000 loss: 0.0011471989039895582\n",
      "  batch 16500 loss: 0.00019229984604978155\n",
      "  batch 17000 loss: 0.01141160062621778\n",
      "  batch 17500 loss: 0.000994491923617268\n",
      "  batch 18000 loss: 0.005668103284884182\n",
      "  batch 18500 loss: 0.0038855918907029583\n",
      "  batch 19000 loss: 0.006198199339556058\n",
      "  batch 19500 loss: 0.0008531669942850684\n",
      "  batch 20000 loss: 0.010482329768099579\n",
      "  batch 20500 loss: 0.0049975539552289125\n",
      "  batch 21000 loss: 0.009541699413870376\n",
      "  batch 21500 loss: 0.0007254664887832369\n",
      "  batch 22000 loss: 0.001322183220244355\n",
      "  batch 22500 loss: 0.003419184108861124\n",
      "  batch 23000 loss: 0.00423535050366679\n",
      "  batch 23500 loss: 0.0005805182423579503\n",
      "  batch 24000 loss: 0.0021646688770078555\n",
      "  batch 24500 loss: 0.000662488918237127\n",
      "  batch 25000 loss: 0.00019993050819294922\n",
      "  batch 25500 loss: 0.00026965723368070724\n",
      "  batch 26000 loss: 0.00025980581874317465\n",
      "  batch 26500 loss: 0.0012245225355916709\n",
      "  batch 27000 loss: 0.00015864792713952625\n",
      "LOSS train 0.00015864792713952625 valid 0.0020194018770928507\n",
      "EPOCH 20:\n",
      "  batch 500 loss: 0.0012146225152032209\n",
      "  batch 1000 loss: 0.0005196337360985339\n",
      "  batch 1500 loss: 0.000501524275314349\n",
      "  batch 2000 loss: 0.0002281254352221076\n",
      "  batch 2500 loss: 0.0002496842459804753\n",
      "  batch 3000 loss: 0.0012205519995249397\n",
      "  batch 3500 loss: 0.0020420072955719135\n",
      "  batch 4000 loss: 0.0032150255509096047\n",
      "  batch 4500 loss: 0.000697917907243216\n",
      "  batch 5000 loss: 0.00018815965221089925\n",
      "  batch 5500 loss: 0.0002830448179492322\n",
      "  batch 6000 loss: 0.00022125393278452066\n",
      "  batch 6500 loss: 0.0002659993211512024\n",
      "  batch 7000 loss: 0.00013522458237161318\n",
      "  batch 7500 loss: 0.0020232976647607793\n",
      "  batch 8000 loss: 0.0003964237906006538\n",
      "  batch 8500 loss: 0.001263320999739161\n",
      "  batch 9000 loss: 0.007715585528264896\n",
      "  batch 9500 loss: 0.00048062431848994793\n",
      "  batch 10000 loss: 0.0002775718504762139\n",
      "  batch 10500 loss: 0.0063565086646603635\n",
      "  batch 11000 loss: 0.0002941499878278542\n",
      "  batch 11500 loss: 0.004733622231919465\n",
      "  batch 12000 loss: 0.008221846107595226\n",
      "  batch 12500 loss: 0.010574036582289217\n",
      "  batch 13000 loss: 0.002895298623003054\n",
      "  batch 13500 loss: 0.00047368157045099226\n",
      "  batch 14000 loss: 0.001265108675856517\n",
      "  batch 14500 loss: 0.00017385027657139672\n",
      "  batch 15000 loss: 0.0002773383274647685\n",
      "  batch 15500 loss: 0.004642653449705108\n",
      "  batch 16000 loss: 0.0027128596154002197\n",
      "  batch 16500 loss: 0.005119268342288596\n",
      "  batch 17000 loss: 0.0067813863350297795\n",
      "  batch 17500 loss: 0.003354202801756536\n",
      "  batch 18000 loss: 0.015207679161399106\n",
      "  batch 18500 loss: 0.006951116243466675\n",
      "  batch 19000 loss: 0.0002258521852274349\n",
      "  batch 19500 loss: 0.0005624929106768981\n",
      "  batch 20000 loss: 0.0011202862593930368\n",
      "  batch 20500 loss: 0.0003754700839413694\n",
      "  batch 21000 loss: 0.0011425827005647556\n",
      "  batch 21500 loss: 0.0024897177406171416\n",
      "  batch 22000 loss: 0.01564000059960205\n",
      "  batch 22500 loss: 0.0021912046633969524\n",
      "  batch 23000 loss: 0.0005404676765605139\n",
      "  batch 23500 loss: 0.0005500054118924922\n",
      "  batch 24000 loss: 0.009949677503838504\n",
      "  batch 24500 loss: 0.0010631431675704732\n",
      "  batch 25000 loss: 0.009794465169258726\n",
      "  batch 25500 loss: 0.006475379191443569\n",
      "  batch 26000 loss: 0.0014458088329216672\n",
      "  batch 26500 loss: 0.0009484879828755411\n",
      "  batch 27000 loss: 0.000541256639865054\n",
      "LOSS train 0.000541256639865054 valid 0.0019030632633847604\n",
      "EPOCH 21:\n",
      "  batch 500 loss: 0.00043679025640390277\n",
      "  batch 1000 loss: 0.004135942612718558\n",
      "  batch 1500 loss: 0.0013134321930651006\n",
      "  batch 2000 loss: 0.00027604973482330664\n",
      "  batch 2500 loss: 0.0004015758680395187\n",
      "  batch 3000 loss: 0.00027316341693102333\n",
      "  batch 3500 loss: 0.00303185739486711\n",
      "  batch 4000 loss: 0.0018791477621058802\n",
      "  batch 4500 loss: 0.0009931625569031333\n",
      "  batch 5000 loss: 0.00023106835425033267\n",
      "  batch 5500 loss: 0.00113763945416488\n",
      "  batch 6000 loss: 0.00042985070618676603\n",
      "  batch 6500 loss: 0.01379820130476578\n",
      "  batch 7000 loss: 0.003526904514441778\n",
      "  batch 7500 loss: 0.001390359103155582\n",
      "  batch 8000 loss: 0.0009601651361552257\n",
      "  batch 8500 loss: 0.0014144891662068487\n",
      "  batch 9000 loss: 0.0013064977881521926\n",
      "  batch 9500 loss: 0.0003536512411443979\n",
      "  batch 10000 loss: 0.003234539065905281\n",
      "  batch 10500 loss: 0.001957346631375653\n",
      "  batch 11000 loss: 0.0016639695517238452\n",
      "  batch 11500 loss: 0.0020302500589756607\n",
      "  batch 12000 loss: 0.0004404784620921447\n",
      "  batch 12500 loss: 0.010049940576413618\n",
      "  batch 13000 loss: 0.003297771941654691\n",
      "  batch 13500 loss: 0.0011004384362403386\n",
      "  batch 14000 loss: 0.049413312683722115\n",
      "  batch 14500 loss: 0.013289211393007505\n",
      "  batch 15000 loss: 0.0007189652141094136\n",
      "  batch 15500 loss: 0.0008084334700603577\n",
      "  batch 16000 loss: 0.008923387913567542\n",
      "  batch 16500 loss: 0.003136396192592198\n",
      "  batch 17000 loss: 0.0003464372785080165\n",
      "  batch 17500 loss: 0.0006135988008014692\n",
      "  batch 18000 loss: 0.0004747710921010082\n",
      "  batch 18500 loss: 0.00036202015228366236\n",
      "  batch 19000 loss: 0.0018910756699778196\n",
      "  batch 19500 loss: 0.0003296585119392361\n",
      "  batch 20000 loss: 0.028807083246283224\n",
      "  batch 20500 loss: 0.0012977182814257909\n",
      "  batch 21000 loss: 0.011298784704345099\n",
      "  batch 21500 loss: 0.006452710452964005\n",
      "  batch 22000 loss: 0.0012382703654991688\n",
      "  batch 22500 loss: 0.0004118556593535807\n",
      "  batch 23000 loss: 0.0002465261954529083\n",
      "  batch 23500 loss: 0.00977403425456236\n",
      "  batch 24000 loss: 0.0013502907715983526\n",
      "  batch 24500 loss: 0.0005567830886925478\n",
      "  batch 25000 loss: 0.003934065784229709\n",
      "  batch 25500 loss: 0.002090061356122909\n",
      "  batch 26000 loss: 0.00046630339060278205\n",
      "  batch 26500 loss: 0.0004858145254101096\n",
      "  batch 27000 loss: 0.00044038811343150285\n",
      "LOSS train 0.00044038811343150285 valid 0.0019293670220417965\n",
      "EPOCH 22:\n",
      "  batch 500 loss: 0.00021295970309139278\n",
      "  batch 1000 loss: 0.00032042548771825353\n",
      "  batch 1500 loss: 0.0033236137113779305\n",
      "  batch 2000 loss: 0.004340041861090182\n",
      "  batch 2500 loss: 0.0003592265142693591\n",
      "  batch 3000 loss: 0.0005098135308757286\n",
      "  batch 3500 loss: 0.0001671448912500111\n",
      "  batch 4000 loss: 0.0003126721413083224\n",
      "  batch 4500 loss: 0.0002334065291619787\n",
      "  batch 5000 loss: 0.0002258664027839181\n",
      "  batch 5500 loss: 0.0002027971346740749\n",
      "  batch 6000 loss: 0.00024066931619245224\n",
      "  batch 6500 loss: 0.0033560318082323662\n",
      "  batch 7000 loss: 0.0039620876685707886\n",
      "  batch 7500 loss: 0.0009636240503931717\n",
      "  batch 8000 loss: 0.018875452088138168\n",
      "  batch 8500 loss: 0.0010783629633120313\n",
      "  batch 9000 loss: 0.005933943430230386\n",
      "  batch 9500 loss: 0.0004533700571371142\n",
      "  batch 10000 loss: 0.00041336737834329185\n",
      "  batch 10500 loss: 0.002638663259881373\n",
      "  batch 11000 loss: 0.005100617476716558\n",
      "  batch 11500 loss: 0.004577810008656385\n",
      "  batch 12000 loss: 0.00309897665881903\n",
      "  batch 12500 loss: 0.0011352691877971352\n",
      "  batch 13000 loss: 0.0005393331394732108\n",
      "  batch 13500 loss: 0.0010461729147291017\n",
      "  batch 14000 loss: 0.0009130485427517314\n",
      "  batch 14500 loss: 0.000498686294894334\n",
      "  batch 15000 loss: 0.014070508253165524\n",
      "  batch 15500 loss: 0.0018079965572482166\n",
      "  batch 16000 loss: 0.00017870506889935812\n",
      "  batch 16500 loss: 0.0012979628751092172\n",
      "  batch 17000 loss: 0.0005627698911392578\n",
      "  batch 17500 loss: 0.004441970016787046\n",
      "  batch 18000 loss: 0.0004955527447807526\n",
      "  batch 18500 loss: 0.0002812948365441947\n",
      "  batch 19000 loss: 0.00043526315214680625\n",
      "  batch 19500 loss: 0.00047647205274689244\n",
      "  batch 20000 loss: 0.00027047130222862135\n",
      "  batch 20500 loss: 0.00037606585371483805\n",
      "  batch 21000 loss: 0.0017076490031238656\n",
      "  batch 21500 loss: 0.00032027388850993787\n",
      "  batch 22000 loss: 0.001043337702314453\n",
      "  batch 22500 loss: 0.0009914711755462733\n",
      "  batch 23000 loss: 0.019601608809681932\n",
      "  batch 23500 loss: 0.0005428909537174107\n",
      "  batch 24000 loss: 0.011230243410477453\n",
      "  batch 24500 loss: 0.0035264161187745485\n",
      "  batch 25000 loss: 0.0008283894249256818\n",
      "  batch 25500 loss: 0.0002765630907128944\n",
      "  batch 26000 loss: 0.0006321702170665731\n",
      "  batch 26500 loss: 0.00014899017553771543\n",
      "  batch 27000 loss: 0.007802182245117777\n",
      "LOSS train 0.007802182245117777 valid 0.002134444914822416\n",
      "EPOCH 23:\n",
      "  batch 500 loss: 0.0008296553440928882\n",
      "  batch 1000 loss: 0.00027017459129132605\n",
      "  batch 1500 loss: 0.005059243484074901\n",
      "  batch 2000 loss: 0.0031342950232794067\n",
      "  batch 2500 loss: 0.007490888833719335\n",
      "  batch 3000 loss: 0.001079537987499247\n",
      "  batch 3500 loss: 0.002739693718497328\n",
      "  batch 4000 loss: 0.0008960210066134451\n",
      "  batch 4500 loss: 0.0002452304557357081\n",
      "  batch 5000 loss: 0.00039237603777755935\n",
      "  batch 5500 loss: 0.0016359729719766882\n",
      "  batch 6000 loss: 0.00024337915252081998\n",
      "  batch 6500 loss: 0.0073648899949521965\n",
      "  batch 7000 loss: 0.0012124244537734653\n",
      "  batch 7500 loss: 0.0013094551650266837\n",
      "  batch 8000 loss: 0.00017801778618690633\n",
      "  batch 8500 loss: 0.0012219949160281302\n",
      "  batch 9000 loss: 0.0004899091686207306\n",
      "  batch 9500 loss: 0.00019882109883640454\n",
      "  batch 10000 loss: 0.00125063654387165\n",
      "  batch 10500 loss: 0.0007243089952122475\n",
      "  batch 11000 loss: 0.0003974385992478915\n",
      "  batch 11500 loss: 0.0001303777566212716\n",
      "  batch 12000 loss: 0.00018494572236420835\n",
      "  batch 12500 loss: 0.000774478427588118\n",
      "  batch 13000 loss: 0.009333185356860466\n",
      "  batch 13500 loss: 0.00458855494632395\n",
      "  batch 14000 loss: 0.0009283897985292171\n",
      "  batch 14500 loss: 0.0009572263743811469\n",
      "  batch 15000 loss: 0.010868860142233415\n",
      "  batch 15500 loss: 0.0006499791966618779\n",
      "  batch 16000 loss: 0.00037161511004785906\n",
      "  batch 16500 loss: 0.0010836649615325379\n",
      "  batch 17000 loss: 0.007786321312310015\n",
      "  batch 17500 loss: 0.003636449201335818\n",
      "  batch 18000 loss: 0.000585086605650325\n",
      "  batch 18500 loss: 0.0005100487484105685\n",
      "  batch 19000 loss: 0.00013865316114255321\n",
      "  batch 19500 loss: 0.00021803615806413034\n",
      "  batch 20000 loss: 0.0016465346295820033\n",
      "  batch 20500 loss: 0.0006374433131882995\n",
      "  batch 21000 loss: 0.00025003614089373014\n",
      "  batch 21500 loss: 0.00540878863274796\n",
      "  batch 22000 loss: 0.0013095528392359909\n",
      "  batch 22500 loss: 0.020763924996508084\n",
      "  batch 23000 loss: 0.0005119977568808558\n",
      "  batch 23500 loss: 0.0005300058493684432\n",
      "  batch 24000 loss: 0.0035831198725323043\n",
      "  batch 24500 loss: 0.0002683166924034097\n",
      "  batch 25000 loss: 0.00024207677597482303\n",
      "  batch 25500 loss: 0.0015790612125813012\n",
      "  batch 26000 loss: 0.002120020132329152\n",
      "  batch 26500 loss: 0.0013363470390033321\n",
      "  batch 27000 loss: 0.005676520329949313\n",
      "LOSS train 0.005676520329949313 valid 0.0017171372783972318\n",
      "EPOCH 24:\n",
      "  batch 500 loss: 0.0008655911722112393\n",
      "  batch 1000 loss: 0.0003298694165170417\n",
      "  batch 1500 loss: 0.005523522744440015\n",
      "  batch 2000 loss: 0.002474136699019599\n",
      "  batch 2500 loss: 0.0043293566167728345\n",
      "  batch 3000 loss: 0.006838246096312215\n",
      "  batch 3500 loss: 0.0038038737499738\n",
      "  batch 4000 loss: 0.0019514343691337359\n",
      "  batch 4500 loss: 0.0007050982262424341\n",
      "  batch 5000 loss: 0.0007186715859706077\n",
      "  batch 5500 loss: 0.0009509461140433295\n",
      "  batch 6000 loss: 0.00029120197394242095\n",
      "  batch 6500 loss: 0.0007832228367047272\n",
      "  batch 7000 loss: 0.00036509485698991284\n",
      "  batch 7500 loss: 0.0005368170947723385\n",
      "  batch 8000 loss: 0.00016185131874746616\n",
      "  batch 8500 loss: 0.00034077832835424714\n",
      "  batch 9000 loss: 0.00019473202800060108\n",
      "  batch 9500 loss: 0.0002917095726643524\n",
      "  batch 10000 loss: 0.00016746492802607803\n",
      "  batch 10500 loss: 0.007397128827030453\n",
      "  batch 11000 loss: 0.0033976557031790582\n",
      "  batch 11500 loss: 0.0013048606908730954\n",
      "  batch 12000 loss: 0.003394591704342126\n",
      "  batch 12500 loss: 0.0006945156378335647\n",
      "  batch 13000 loss: 0.0004711910494823144\n",
      "  batch 13500 loss: 0.00028345912116775553\n",
      "  batch 14000 loss: 0.0001815958576360579\n",
      "  batch 14500 loss: 0.00031124058120407126\n",
      "  batch 15000 loss: 0.00032958199575188016\n",
      "  batch 15500 loss: 0.00015603776331280983\n",
      "  batch 16000 loss: 0.0001516499357611174\n",
      "  batch 16500 loss: 0.008534679005558612\n",
      "  batch 17000 loss: 0.000538184758810452\n",
      "  batch 17500 loss: 0.0012962658868529288\n",
      "  batch 18000 loss: 0.0004770900422933728\n",
      "  batch 18500 loss: 0.000456850533259562\n",
      "  batch 19000 loss: 0.0007351395764302104\n",
      "  batch 19500 loss: 0.00027428351111061743\n",
      "  batch 20000 loss: 0.0012164399675776564\n",
      "  batch 20500 loss: 0.009940507674465398\n",
      "  batch 21000 loss: 0.0006234524173490747\n",
      "  batch 21500 loss: 0.011232431892140458\n",
      "  batch 22000 loss: 0.01475509001475978\n",
      "  batch 22500 loss: 0.015078559860182363\n",
      "  batch 23000 loss: 0.002719420534716306\n",
      "  batch 23500 loss: 0.0010168817270581555\n",
      "  batch 24000 loss: 0.0007342660346890249\n",
      "  batch 24500 loss: 0.0005510610838026757\n",
      "  batch 25000 loss: 0.00041330923295286224\n",
      "  batch 25500 loss: 0.000326799831289204\n",
      "  batch 26000 loss: 0.0007828162480506932\n",
      "  batch 26500 loss: 0.00030994877106102335\n",
      "  batch 27000 loss: 0.00028327201978565596\n",
      "LOSS train 0.00028327201978565596 valid 0.0016421016645424564\n",
      "EPOCH 25:\n",
      "  batch 500 loss: 0.00025041710625589174\n",
      "  batch 1000 loss: 0.0033209815444237094\n",
      "  batch 1500 loss: 0.00016777783233158417\n",
      "  batch 2000 loss: 0.004803398346184575\n",
      "  batch 2500 loss: 0.00024059211999972874\n",
      "  batch 3000 loss: 0.005110614847572237\n",
      "  batch 3500 loss: 0.001408494954124695\n",
      "  batch 4000 loss: 0.0023138578143456455\n",
      "  batch 4500 loss: 0.0015054006115265608\n",
      "  batch 5000 loss: 0.00040366000977651594\n",
      "  batch 5500 loss: 0.0008806251777388993\n",
      "  batch 6000 loss: 0.00018782248192919228\n",
      "  batch 6500 loss: 0.0003102907186020154\n",
      "  batch 7000 loss: 0.000484676156161008\n",
      "  batch 7500 loss: 0.01465284258334944\n",
      "  batch 8000 loss: 0.0010442355897485526\n",
      "  batch 8500 loss: 0.0006428812009167793\n",
      "  batch 9000 loss: 0.00030668646502331496\n",
      "  batch 9500 loss: 0.00017443394196535778\n",
      "  batch 10000 loss: 0.00020975087871384445\n",
      "  batch 10500 loss: 0.00011166749236026874\n",
      "  batch 11000 loss: 0.00013853668164002286\n",
      "  batch 11500 loss: 0.0004313547099711883\n",
      "  batch 12000 loss: 0.0005943707142100187\n",
      "  batch 12500 loss: 0.0004180045430268464\n",
      "  batch 13000 loss: 0.0018512388506339264\n",
      "  batch 13500 loss: 0.0017979352579087298\n",
      "  batch 14000 loss: 0.017115441574765192\n",
      "  batch 14500 loss: 0.0004433530076701437\n",
      "  batch 15000 loss: 0.00030545262707454767\n",
      "  batch 15500 loss: 0.0002873515712710777\n",
      "  batch 16000 loss: 0.00026181953890181477\n",
      "  batch 16500 loss: 0.0003652558459595525\n",
      "  batch 17000 loss: 0.0034226117935786176\n",
      "  batch 17500 loss: 0.0007512157912501038\n",
      "  batch 18000 loss: 0.0035902717496168464\n",
      "  batch 18500 loss: 0.0011296530070099742\n",
      "  batch 19000 loss: 0.004526829021365689\n",
      "  batch 19500 loss: 0.0003458222522184009\n",
      "  batch 20000 loss: 0.0005668207436630297\n",
      "  batch 20500 loss: 0.0042832123025781\n",
      "  batch 21000 loss: 0.0008062094154855544\n",
      "  batch 21500 loss: 0.00048616866390891643\n",
      "  batch 22000 loss: 0.0002853639108581305\n",
      "  batch 22500 loss: 0.0015701375769144725\n",
      "  batch 23000 loss: 0.0023838356819910656\n",
      "  batch 23500 loss: 0.0002603177139167592\n",
      "  batch 24000 loss: 0.0007275798684651775\n",
      "  batch 24500 loss: 0.00019347304587671133\n",
      "  batch 25000 loss: 0.0004158017167123056\n",
      "  batch 25500 loss: 0.00020378722838304596\n",
      "  batch 26000 loss: 0.00023698443703325012\n",
      "  batch 26500 loss: 0.00018737054654735985\n",
      "  batch 27000 loss: 0.004077220364485719\n",
      "LOSS train 0.004077220364485719 valid 0.0033286843430528714\n",
      "EPOCH 26:\n",
      "  batch 500 loss: 0.006096922596761317\n",
      "  batch 1000 loss: 0.0010133266744452988\n",
      "  batch 1500 loss: 0.00045081444668522154\n",
      "  batch 2000 loss: 0.0005912027611694874\n",
      "  batch 2500 loss: 0.006100185320820255\n",
      "  batch 3000 loss: 0.0009748407900207638\n",
      "  batch 3500 loss: 0.00028757518458772324\n",
      "  batch 4000 loss: 0.0024967541511694514\n",
      "  batch 4500 loss: 0.011029345614786806\n",
      "  batch 5000 loss: 0.00020803821149405978\n",
      "  batch 5500 loss: 0.0001502917791729814\n",
      "  batch 6000 loss: 0.00043666268683539047\n",
      "  batch 6500 loss: 0.010124341819940905\n",
      "  batch 7000 loss: 0.004379237089972705\n",
      "  batch 7500 loss: 0.0008232562477771488\n",
      "  batch 8000 loss: 0.0008577719416132048\n",
      "  batch 8500 loss: 0.00025755837942048745\n",
      "  batch 9000 loss: 0.0006594916383506693\n",
      "  batch 9500 loss: 0.0038368796802903766\n",
      "  batch 10000 loss: 0.0007873137832651409\n",
      "  batch 10500 loss: 0.0004714156555535105\n",
      "  batch 11000 loss: 0.0005418135018302692\n",
      "  batch 11500 loss: 0.00028540353819499485\n",
      "  batch 12000 loss: 0.00019214726043802698\n",
      "  batch 12500 loss: 0.0001272987121174971\n",
      "  batch 13000 loss: 0.0006220587470491843\n",
      "  batch 13500 loss: 0.00016475280417479254\n",
      "  batch 14000 loss: 0.00019085241348202331\n",
      "  batch 14500 loss: 0.0017043860466846787\n",
      "  batch 15000 loss: 0.0036716655452282013\n",
      "  batch 15500 loss: 0.000502749101313551\n",
      "  batch 16000 loss: 0.0005297739488993791\n",
      "  batch 16500 loss: 0.0003770781789662081\n",
      "  batch 17000 loss: 0.00024059060679199718\n",
      "  batch 17500 loss: 0.004360557611641344\n",
      "  batch 18000 loss: 0.0004188917314179932\n",
      "  batch 18500 loss: 0.0008269536842616709\n",
      "  batch 19000 loss: 0.0003065581500656549\n",
      "  batch 19500 loss: 0.0001773615346545796\n",
      "  batch 20000 loss: 0.0002482677015132246\n",
      "  batch 20500 loss: 0.0014190786029078559\n",
      "  batch 21000 loss: 0.0003358300680299706\n",
      "  batch 21500 loss: 0.00018361726966191583\n",
      "  batch 22000 loss: 0.0003344985322820513\n",
      "  batch 22500 loss: 0.00011699384177784821\n",
      "  batch 23000 loss: 8.424925875094402e-05\n",
      "  batch 23500 loss: 0.00017163084708226605\n",
      "  batch 24000 loss: 0.0004010708595590913\n",
      "  batch 24500 loss: 0.00022790476497916147\n",
      "  batch 25000 loss: 0.0050042839725653035\n",
      "  batch 25500 loss: 0.0001368498605991455\n",
      "  batch 26000 loss: 0.00014866113110783985\n",
      "  batch 26500 loss: 0.0004339191682375301\n",
      "  batch 27000 loss: 0.0003696585070325504\n",
      "LOSS train 0.0003696585070325504 valid 0.0016312672946156728\n",
      "EPOCH 27:\n",
      "  batch 500 loss: 0.017881397512607394\n",
      "  batch 1000 loss: 0.0010843581716436113\n",
      "  batch 1500 loss: 0.022349423817200818\n",
      "  batch 2000 loss: 0.007313911175331761\n",
      "  batch 2500 loss: 0.0014991689808214908\n",
      "  batch 3000 loss: 0.002310914938179909\n",
      "  batch 3500 loss: 0.0002792119341739685\n",
      "  batch 4000 loss: 0.00037389191525195284\n",
      "  batch 4500 loss: 0.00023700171931183433\n",
      "  batch 5000 loss: 0.0002706830893619063\n",
      "  batch 5500 loss: 0.0021795346421715927\n",
      "  batch 6000 loss: 0.00014400940683182738\n",
      "  batch 6500 loss: 0.00048008190359608705\n",
      "  batch 7000 loss: 0.002482315250607943\n",
      "  batch 7500 loss: 0.0006599598016907713\n",
      "  batch 8000 loss: 0.0018910574548123016\n",
      "  batch 8500 loss: 0.00022987693920581976\n",
      "  batch 9000 loss: 0.0012425122701494296\n",
      "  batch 9500 loss: 0.0018303806526534885\n",
      "  batch 10000 loss: 0.0004144631324985646\n",
      "  batch 10500 loss: 0.0002825395278169154\n",
      "  batch 11000 loss: 0.000262007650423147\n",
      "  batch 11500 loss: 0.003322354583102474\n",
      "  batch 12000 loss: 0.0018916780170947262\n",
      "  batch 12500 loss: 0.00038484907346158083\n",
      "  batch 13000 loss: 0.0001665239749935914\n",
      "  batch 13500 loss: 0.00015679075471989633\n",
      "  batch 14000 loss: 0.00012189782925439019\n",
      "  batch 14500 loss: 0.0001756685624911647\n",
      "  batch 15000 loss: 0.001356475551998809\n",
      "  batch 15500 loss: 9.013593374411144e-05\n",
      "  batch 16000 loss: 0.0003322876732368272\n",
      "  batch 16500 loss: 0.00021429565541793494\n",
      "  batch 17000 loss: 0.00010316619373082859\n",
      "  batch 17500 loss: 0.001347076615572206\n",
      "  batch 18000 loss: 0.004928904893612472\n",
      "  batch 18500 loss: 0.0029808252575691087\n",
      "  batch 19000 loss: 0.002096287083436831\n",
      "  batch 19500 loss: 0.0005439139082198708\n",
      "  batch 20000 loss: 0.0005254116562836621\n",
      "  batch 20500 loss: 0.00031615443165105363\n",
      "  batch 21000 loss: 0.00012824887483800396\n",
      "  batch 21500 loss: 0.00014697876734717498\n",
      "  batch 22000 loss: 7.445342159259383e-05\n",
      "  batch 22500 loss: 0.0043849154463763895\n",
      "  batch 23000 loss: 0.005593873930821079\n",
      "  batch 23500 loss: 0.00027641597154730844\n",
      "  batch 24000 loss: 0.01385612343663804\n",
      "  batch 24500 loss: 0.0017043486124879764\n",
      "  batch 25000 loss: 0.007268305377526616\n",
      "  batch 25500 loss: 0.0010773225946885709\n",
      "  batch 26000 loss: 0.00021161362359494262\n",
      "  batch 26500 loss: 0.00016494622281007664\n",
      "  batch 27000 loss: 0.00041572852868773327\n",
      "LOSS train 0.00041572852868773327 valid 0.0017006179094499769\n",
      "EPOCH 28:\n",
      "  batch 500 loss: 0.000744081433680364\n",
      "  batch 1000 loss: 0.02318918342298857\n",
      "  batch 1500 loss: 0.004683890294176713\n",
      "  batch 2000 loss: 0.0015784535207168204\n",
      "  batch 2500 loss: 0.0002378569263783632\n",
      "  batch 3000 loss: 0.008915372352481092\n",
      "  batch 3500 loss: 0.0014848513198734991\n",
      "  batch 4000 loss: 0.00017550934698228103\n",
      "  batch 4500 loss: 0.00023105992027447542\n",
      "  batch 5000 loss: 0.0015241802786021593\n",
      "  batch 5500 loss: 0.013821009585947274\n",
      "  batch 6000 loss: 0.0026307089110625768\n",
      "  batch 6500 loss: 0.00023054693790155767\n",
      "  batch 7000 loss: 0.0008683046489996862\n",
      "  batch 7500 loss: 0.00021628699216323354\n",
      "  batch 8000 loss: 0.00012451373461779623\n",
      "  batch 8500 loss: 0.00016754235240956205\n",
      "  batch 9000 loss: 0.004468020535073741\n",
      "  batch 9500 loss: 0.0007106451664872146\n",
      "  batch 10000 loss: 0.00018865718237287156\n",
      "  batch 10500 loss: 0.006000512491783286\n",
      "  batch 11000 loss: 0.0006717147259300269\n",
      "  batch 11500 loss: 0.00025518813600677604\n",
      "  batch 12000 loss: 0.00024663162193051135\n",
      "  batch 12500 loss: 0.0001517289888455835\n",
      "  batch 13000 loss: 0.0001018651519538416\n",
      "  batch 13500 loss: 0.00029382973741552476\n",
      "  batch 14000 loss: 0.0002860796412364159\n",
      "  batch 14500 loss: 0.008703902269114149\n",
      "  batch 15000 loss: 0.01667508482534492\n",
      "  batch 15500 loss: 0.0005840567402339687\n",
      "  batch 16000 loss: 0.00074611535395875\n",
      "  batch 16500 loss: 0.00017977749083920004\n",
      "  batch 17000 loss: 0.00016909010944742776\n",
      "  batch 17500 loss: 0.0013653659920288526\n",
      "  batch 18000 loss: 0.0002661231715090935\n",
      "  batch 18500 loss: 0.0005410621380595196\n",
      "  batch 19000 loss: 0.000838845070499751\n",
      "  batch 19500 loss: 0.00425589980882949\n",
      "  batch 20000 loss: 0.001163975577089051\n",
      "  batch 20500 loss: 0.00013480269766817443\n",
      "  batch 21000 loss: 0.0011971445000517314\n",
      "  batch 21500 loss: 0.00010663170787697496\n",
      "  batch 22000 loss: 0.0016582302117305937\n",
      "  batch 22500 loss: 0.00026134691905216556\n",
      "  batch 23000 loss: 9.514371153426637e-05\n",
      "  batch 23500 loss: 0.00023146635566094175\n",
      "  batch 24000 loss: 0.0003293103356199154\n",
      "  batch 24500 loss: 0.00027929621085039715\n",
      "  batch 25000 loss: 0.00025994735529794255\n",
      "  batch 25500 loss: 0.0009670343426242987\n",
      "  batch 26000 loss: 0.004593024575671844\n",
      "  batch 26500 loss: 0.00034980914439902125\n",
      "  batch 27000 loss: 0.010580132887808926\n",
      "LOSS train 0.010580132887808926 valid 0.0016569864993849226\n",
      "EPOCH 29:\n",
      "  batch 500 loss: 0.00029593960536703536\n",
      "  batch 1000 loss: 0.0004981723809903471\n",
      "  batch 1500 loss: 0.00024054803446074403\n",
      "  batch 2000 loss: 0.00019004571077168463\n",
      "  batch 2500 loss: 0.003406550174147668\n",
      "  batch 3000 loss: 0.0004620752377592545\n",
      "  batch 3500 loss: 0.00015616290074051874\n",
      "  batch 4000 loss: 0.0010274967356865723\n",
      "  batch 4500 loss: 0.00015987171004682566\n",
      "  batch 5000 loss: 0.00011641028448877578\n",
      "  batch 5500 loss: 0.001053453291278604\n",
      "  batch 6000 loss: 0.006504258566202097\n",
      "  batch 6500 loss: 0.0003538984247335648\n",
      "  batch 7000 loss: 0.0003809892121168517\n",
      "  batch 7500 loss: 0.00024433653929292374\n",
      "  batch 8000 loss: 0.0004249889885464242\n",
      "  batch 8500 loss: 0.0003124408999684505\n",
      "  batch 9000 loss: 0.0006178732502257134\n",
      "  batch 9500 loss: 0.00011453067794084504\n",
      "  batch 10000 loss: 0.0006369172423719895\n",
      "  batch 10500 loss: 0.00034651023590780025\n",
      "  batch 11000 loss: 0.000977700895936561\n",
      "  batch 11500 loss: 0.000389178163981466\n",
      "  batch 12000 loss: 0.000169761818650489\n",
      "  batch 12500 loss: 0.0006015420054861877\n",
      "  batch 13000 loss: 0.002987683975143678\n",
      "  batch 13500 loss: 0.00018034271232423294\n",
      "  batch 14000 loss: 0.0002024828854613041\n",
      "  batch 14500 loss: 0.0003099709264859243\n",
      "  batch 15000 loss: 0.00010826857189744387\n",
      "  batch 15500 loss: 0.009348977312367755\n",
      "  batch 16000 loss: 0.0005787278582341244\n",
      "  batch 16500 loss: 0.0007576338712354058\n",
      "  batch 17000 loss: 0.002874277556372384\n",
      "  batch 17500 loss: 0.00020262917968048554\n",
      "  batch 18000 loss: 0.0003401841730878701\n",
      "  batch 18500 loss: 0.00047420047891283445\n",
      "  batch 19000 loss: 0.00018538875423318048\n",
      "  batch 19500 loss: 0.00013159008102849157\n",
      "  batch 20000 loss: 0.00024895876035343887\n",
      "  batch 20500 loss: 0.00012780725543344573\n",
      "  batch 21000 loss: 8.511857633682852e-05\n",
      "  batch 21500 loss: 8.283723562945511e-05\n",
      "  batch 22000 loss: 0.00011896037272633108\n",
      "  batch 22500 loss: 4.06861138701835e-05\n",
      "  batch 23000 loss: 7.260838201320042e-05\n",
      "  batch 23500 loss: 0.0004942418835140252\n",
      "  batch 24000 loss: 0.0012897485124003013\n",
      "  batch 24500 loss: 0.0005723987667359296\n",
      "  batch 25000 loss: 0.0006394486169085027\n",
      "  batch 25500 loss: 0.0006054840763004883\n",
      "  batch 26000 loss: 0.008319375502597506\n",
      "  batch 26500 loss: 0.00016494228926138276\n",
      "  batch 27000 loss: 0.016256068273192197\n",
      "LOSS train 0.016256068273192197 valid 0.002074141681794433\n",
      "EPOCH 30:\n",
      "  batch 500 loss: 0.0003019401689281054\n",
      "  batch 1000 loss: 0.003799784140997598\n",
      "  batch 1500 loss: 0.0002486713040148274\n",
      "  batch 2000 loss: 0.0011284603592195098\n",
      "  batch 2500 loss: 0.00041272215261035683\n",
      "  batch 3000 loss: 0.0003103804487886492\n",
      "  batch 3500 loss: 0.0004124955616071837\n",
      "  batch 4000 loss: 0.0024847535564890464\n",
      "  batch 4500 loss: 0.00046582380527749836\n",
      "  batch 5000 loss: 0.0022950932909116374\n",
      "  batch 5500 loss: 0.00023698128591381363\n",
      "  batch 6000 loss: 0.000178922304020098\n",
      "  batch 6500 loss: 0.0007801776998302544\n",
      "  batch 7000 loss: 0.008100942800250295\n",
      "  batch 7500 loss: 0.00011709325972658391\n",
      "  batch 8000 loss: 0.00014646210105490097\n",
      "  batch 8500 loss: 6.751496729331308e-05\n",
      "  batch 9000 loss: 0.0006355723952732894\n",
      "  batch 9500 loss: 0.0006230299472978054\n",
      "  batch 10000 loss: 0.0032809872806189625\n",
      "  batch 10500 loss: 0.0033264112065620284\n",
      "  batch 11000 loss: 0.00043089044613601145\n",
      "  batch 11500 loss: 0.0049714102540924475\n",
      "  batch 12000 loss: 0.0005443286741313749\n",
      "  batch 12500 loss: 8.543213420595919e-05\n",
      "  batch 13000 loss: 0.0009725342780195127\n",
      "  batch 13500 loss: 0.00013480210995590625\n",
      "  batch 14000 loss: 0.0001595992814614746\n",
      "  batch 14500 loss: 0.0005389121245415076\n",
      "  batch 15000 loss: 0.0001412791427847324\n",
      "  batch 15500 loss: 0.00022383137461144998\n",
      "  batch 16000 loss: 0.00026586877025003373\n",
      "  batch 16500 loss: 0.0003101859601845334\n",
      "  batch 17000 loss: 9.645901451867544e-05\n",
      "  batch 17500 loss: 0.0041723531534315515\n",
      "  batch 18000 loss: 0.0004535706372998369\n",
      "  batch 18500 loss: 0.0006868587142525832\n",
      "  batch 19000 loss: 0.0001720503445990893\n",
      "  batch 19500 loss: 6.448472236608538e-05\n",
      "  batch 20000 loss: 0.00017889745556644244\n",
      "  batch 20500 loss: 0.00017586077207589311\n",
      "  batch 21000 loss: 0.0005323596204318904\n",
      "  batch 21500 loss: 0.01814338754481054\n",
      "  batch 22000 loss: 0.0030014810321790826\n",
      "  batch 22500 loss: 0.00045648355990832103\n",
      "  batch 23000 loss: 0.002707452567055256\n",
      "  batch 23500 loss: 0.0008048062382300323\n",
      "  batch 24000 loss: 0.0005952405507973175\n",
      "  batch 24500 loss: 0.000124478021770571\n",
      "  batch 25000 loss: 0.0004737959956401632\n",
      "  batch 25500 loss: 0.00030686811561596625\n",
      "  batch 26000 loss: 0.0001610735258531193\n",
      "  batch 26500 loss: 0.0001157202459698432\n",
      "  batch 27000 loss: 0.00018156335914340716\n",
      "LOSS train 0.00018156335914340716 valid 0.0015078836488652179\n",
      "EPOCH 31:\n",
      "  batch 500 loss: 0.00010542623844205323\n",
      "  batch 1000 loss: 0.0004211448858655089\n",
      "  batch 1500 loss: 0.00017885820174687694\n",
      "  batch 2000 loss: 0.00013120452575646625\n",
      "  batch 2500 loss: 0.000130470929404769\n",
      "  batch 3000 loss: 0.00011983922233961764\n",
      "  batch 3500 loss: 0.00021140154577696535\n",
      "  batch 4000 loss: 0.00015687494603820084\n",
      "  batch 4500 loss: 0.0002656973432176528\n",
      "  batch 5000 loss: 0.00014462184480799678\n",
      "  batch 5500 loss: 0.00015485978386837473\n",
      "  batch 6000 loss: 0.00010471583864895351\n",
      "  batch 6500 loss: 0.00020504507977022257\n",
      "  batch 7000 loss: 0.00017669763070795596\n",
      "  batch 7500 loss: 0.0002880394276962974\n",
      "  batch 8000 loss: 0.00029815619758906566\n",
      "  batch 8500 loss: 0.0017982501480267885\n",
      "  batch 9000 loss: 0.000182599202633682\n",
      "  batch 9500 loss: 0.02794995059533487\n",
      "  batch 10000 loss: 0.0007043604380173178\n",
      "  batch 10500 loss: 0.0016839151484034289\n",
      "  batch 11000 loss: 0.00023314401907934722\n",
      "  batch 11500 loss: 0.00014318008348583076\n",
      "  batch 12000 loss: 0.00034660375417254133\n",
      "  batch 12500 loss: 0.0005366600651901373\n",
      "  batch 13000 loss: 0.00019476521290957648\n",
      "  batch 13500 loss: 0.0012417531289127942\n",
      "  batch 14000 loss: 0.0005273569181951139\n",
      "  batch 14500 loss: 0.0005651073826924353\n",
      "  batch 15000 loss: 0.0004041243930706244\n",
      "  batch 15500 loss: 0.001884717847600662\n",
      "  batch 16000 loss: 0.0004272369394995188\n",
      "  batch 16500 loss: 0.0021628785491811867\n",
      "  batch 17000 loss: 0.0028463324446921343\n",
      "  batch 17500 loss: 0.0053971064087879736\n",
      "  batch 18000 loss: 0.001343378735278641\n",
      "  batch 18500 loss: 0.0006186360762614456\n",
      "  batch 19000 loss: 0.0017828885426085996\n",
      "  batch 19500 loss: 0.0003828792024069436\n",
      "  batch 20000 loss: 0.00012552228752140237\n",
      "  batch 20500 loss: 0.0020628426558384413\n",
      "  batch 21000 loss: 0.0015633941648281855\n",
      "  batch 21500 loss: 0.0007795284080720925\n",
      "  batch 22000 loss: 0.004423119678075718\n",
      "  batch 22500 loss: 0.0022363448071388843\n",
      "  batch 23000 loss: 0.002429188586287509\n",
      "  batch 23500 loss: 0.00024500054793399786\n",
      "  batch 24000 loss: 0.00020812889229458164\n",
      "  batch 24500 loss: 0.0001289309836963426\n",
      "  batch 25000 loss: 0.0038213972569677212\n",
      "  batch 25500 loss: 0.00830943350955172\n",
      "  batch 26000 loss: 0.0004747567671144566\n",
      "  batch 26500 loss: 0.000125587760216316\n",
      "  batch 27000 loss: 0.0010349711198299864\n",
      "LOSS train 0.0010349711198299864 valid 0.07179539916432852\n",
      "EPOCH 32:\n",
      "  batch 500 loss: 0.0005603093471760054\n",
      "  batch 1000 loss: 0.00014539699111767846\n",
      "  batch 1500 loss: 0.00019475560187003893\n",
      "  batch 2000 loss: 0.0005240050182199454\n",
      "  batch 2500 loss: 0.0004103709832546016\n",
      "  batch 3000 loss: 0.00011242229497027267\n",
      "  batch 3500 loss: 0.0019459617768652607\n",
      "  batch 4000 loss: 0.027579524811320987\n",
      "  batch 4500 loss: 0.0012203664192819979\n",
      "  batch 5000 loss: 0.0003068403471202643\n",
      "  batch 5500 loss: 0.003010412187591193\n",
      "  batch 6000 loss: 0.00015540925389785442\n",
      "  batch 6500 loss: 0.0004908098960526602\n",
      "  batch 7000 loss: 0.00043717496536686086\n",
      "  batch 7500 loss: 0.000663660101318122\n",
      "  batch 8000 loss: 0.007099071410051014\n",
      "  batch 8500 loss: 0.0061724232362075785\n",
      "  batch 9000 loss: 0.002254437464598542\n",
      "  batch 9500 loss: 0.0010586426833094152\n",
      "  batch 10000 loss: 0.015171668946020648\n",
      "  batch 10500 loss: 0.0002567247679382838\n",
      "  batch 11000 loss: 0.0031519563046595173\n",
      "  batch 11500 loss: 0.0005498793592032136\n",
      "  batch 12000 loss: 0.0003091779569422286\n",
      "  batch 12500 loss: 0.0060984226336649495\n",
      "  batch 13000 loss: 0.0004955936404064758\n",
      "  batch 13500 loss: 0.0009058046602429535\n",
      "  batch 14000 loss: 0.004514225584267844\n",
      "  batch 14500 loss: 0.0002169877202670776\n",
      "  batch 15000 loss: 0.00015425048974291043\n",
      "  batch 15500 loss: 0.0002811795992102724\n",
      "  batch 16000 loss: 0.002150096057317818\n",
      "  batch 16500 loss: 0.002069502966076392\n",
      "  batch 17000 loss: 0.005348919416765132\n",
      "  batch 17500 loss: 0.00033011218338218387\n",
      "  batch 18000 loss: 0.003463622315394151\n",
      "  batch 18500 loss: 0.0004213696627454908\n",
      "  batch 19000 loss: 0.0008514260969281331\n",
      "  batch 19500 loss: 0.00013787056994083713\n",
      "  batch 20000 loss: 8.92482850484484e-05\n",
      "  batch 20500 loss: 0.0011503730297879038\n",
      "  batch 21000 loss: 0.0003232269954503799\n",
      "  batch 21500 loss: 0.0023434367286958454\n",
      "  batch 22000 loss: 0.0024739430173558075\n",
      "  batch 22500 loss: 0.00014649447577329156\n",
      "  batch 23000 loss: 0.0017894808195272809\n",
      "  batch 23500 loss: 0.0012369772108252982\n",
      "  batch 24000 loss: 0.00026775419985286677\n",
      "  batch 24500 loss: 0.00017402784899648083\n",
      "  batch 25000 loss: 0.0003606441070341475\n",
      "  batch 25500 loss: 0.0030982659629003777\n",
      "  batch 26000 loss: 0.0008322190057616687\n",
      "  batch 26500 loss: 0.0003302259582434317\n",
      "  batch 27000 loss: 0.00246551916107747\n",
      "LOSS train 0.00246551916107747 valid 0.0018285239425609961\n",
      "EPOCH 33:\n",
      "  batch 500 loss: 0.00020066313702938031\n",
      "  batch 1000 loss: 0.000898499432721028\n",
      "  batch 1500 loss: 0.0023962571647862135\n",
      "  batch 2000 loss: 0.00010250586251472526\n",
      "  batch 2500 loss: 0.0007677951901992692\n",
      "  batch 3000 loss: 0.00011852431839306376\n",
      "  batch 3500 loss: 8.766879027706409e-05\n",
      "  batch 4000 loss: 9.373850052925903e-05\n",
      "  batch 4500 loss: 0.0004662809946786801\n",
      "  batch 5000 loss: 0.01247609086621538\n",
      "  batch 5500 loss: 0.0009109639404338381\n",
      "  batch 6000 loss: 0.0008895461517216034\n",
      "  batch 6500 loss: 0.00022939217058738493\n",
      "  batch 7000 loss: 0.000657742243126517\n",
      "  batch 7500 loss: 0.00030545763521923065\n",
      "  batch 8000 loss: 0.0020220039023678815\n",
      "  batch 8500 loss: 0.0003450128211067103\n",
      "  batch 9000 loss: 0.00049108867793122\n",
      "  batch 9500 loss: 0.00017601884423316605\n",
      "  batch 10000 loss: 0.001436558746804014\n",
      "  batch 10500 loss: 0.00010865348375601514\n",
      "  batch 11000 loss: 0.0002156620485603078\n",
      "  batch 11500 loss: 0.004649486921189538\n",
      "  batch 12000 loss: 0.00021938025939805073\n",
      "  batch 12500 loss: 0.0015812933808684447\n",
      "  batch 13000 loss: 0.005346269824271279\n",
      "  batch 13500 loss: 0.000538259369777407\n",
      "  batch 14000 loss: 0.0012097328297611476\n",
      "  batch 14500 loss: 0.00024330908339906542\n",
      "  batch 15000 loss: 0.00013025401061194232\n",
      "  batch 15500 loss: 0.0004267304024132379\n",
      "  batch 16000 loss: 9.636433313238512e-05\n",
      "  batch 16500 loss: 0.033313950299983505\n",
      "  batch 17000 loss: 0.000791909684428397\n",
      "  batch 17500 loss: 0.01781760632743434\n",
      "  batch 18000 loss: 0.0002516930834388482\n",
      "  batch 18500 loss: 0.00027446154980283665\n",
      "  batch 19000 loss: 0.00016432742236628783\n",
      "  batch 19500 loss: 0.0003093236038541782\n",
      "  batch 20000 loss: 0.0006605643562659509\n",
      "  batch 20500 loss: 0.00390059655644308\n",
      "  batch 21000 loss: 0.0007048503439358917\n",
      "  batch 21500 loss: 0.00023531500526926408\n",
      "  batch 22000 loss: 0.0010102455120307567\n",
      "  batch 22500 loss: 0.0005740061173186426\n",
      "  batch 23000 loss: 0.00132885962919406\n",
      "  batch 23500 loss: 0.0006676262884413796\n",
      "  batch 24000 loss: 0.00013740393691259457\n",
      "  batch 24500 loss: 0.00039213492108436653\n",
      "  batch 25000 loss: 0.0014545231350307582\n",
      "  batch 25500 loss: 0.0001567993302877042\n",
      "  batch 26000 loss: 0.00036001505215967457\n",
      "  batch 26500 loss: 0.0008404623059866232\n",
      "  batch 27000 loss: 0.0007326963476896716\n",
      "LOSS train 0.0007326963476896716 valid 0.001422621831912517\n",
      "EPOCH 34:\n",
      "  batch 500 loss: 0.00010913680371479373\n",
      "  batch 1000 loss: 0.00018059536373847963\n",
      "  batch 1500 loss: 6.997777698367358e-05\n",
      "  batch 2000 loss: 0.0031563030018394614\n",
      "  batch 2500 loss: 0.004752296341581587\n",
      "  batch 3000 loss: 0.0002629765565502069\n",
      "  batch 3500 loss: 0.0007323214281862569\n",
      "  batch 4000 loss: 0.0002531824121910553\n",
      "  batch 4500 loss: 0.0007354598174555953\n",
      "  batch 5000 loss: 0.0004093132618529403\n",
      "  batch 5500 loss: 0.0001143008002755721\n",
      "  batch 6000 loss: 0.0006224721421822466\n",
      "  batch 6500 loss: 0.00010898522047630976\n",
      "  batch 7000 loss: 6.593894269548528e-05\n",
      "  batch 7500 loss: 0.0013521779615705398\n",
      "  batch 8000 loss: 0.012239604008860934\n",
      "  batch 8500 loss: 0.016163173147632336\n",
      "  batch 9000 loss: 0.00020751944576537085\n",
      "  batch 9500 loss: 0.00016066172314252824\n",
      "  batch 10000 loss: 0.001043007787470323\n",
      "  batch 10500 loss: 0.0003757141601868206\n",
      "  batch 11000 loss: 0.0002265525293358479\n",
      "  batch 11500 loss: 0.00011633155575543697\n",
      "  batch 12000 loss: 0.00010575519143785428\n",
      "  batch 12500 loss: 0.00397382068516735\n",
      "  batch 13000 loss: 0.018185523637776972\n",
      "  batch 13500 loss: 0.00016629240527450052\n",
      "  batch 14000 loss: 0.004392455553922875\n",
      "  batch 14500 loss: 0.0004386246301240249\n",
      "  batch 15000 loss: 0.00014595738387182422\n",
      "  batch 15500 loss: 0.00012432681717834982\n",
      "  batch 16000 loss: 0.0002991278371933959\n",
      "  batch 16500 loss: 0.000864177869812412\n",
      "  batch 17000 loss: 0.0003721345618065648\n",
      "  batch 17500 loss: 0.0005384796382457999\n",
      "  batch 18000 loss: 0.00022883838621982734\n",
      "  batch 18500 loss: 8.761993768536058e-05\n",
      "  batch 19000 loss: 0.00013132490084706206\n",
      "  batch 19500 loss: 0.0004412945058694149\n",
      "  batch 20000 loss: 0.002424002711808171\n",
      "  batch 20500 loss: 0.00020963431875991388\n",
      "  batch 21000 loss: 0.0003869206190259078\n",
      "  batch 21500 loss: 0.0009947863311100989\n",
      "  batch 22000 loss: 0.002075911086283401\n",
      "  batch 22500 loss: 0.004410801923579903\n",
      "  batch 23000 loss: 0.0002215522941423842\n",
      "  batch 23500 loss: 0.0005851810927981554\n",
      "  batch 24000 loss: 0.00015439438639759472\n",
      "  batch 24500 loss: 0.0001887239495893631\n",
      "  batch 25000 loss: 0.00032147040054781684\n",
      "  batch 25500 loss: 0.00018638655480309295\n",
      "  batch 26000 loss: 0.0001838636413328203\n",
      "  batch 26500 loss: 0.00011158623416240232\n",
      "  batch 27000 loss: 0.0016544727550321348\n",
      "LOSS train 0.0016544727550321348 valid 0.002165895950773032\n",
      "EPOCH 35:\n",
      "  batch 500 loss: 0.0005799324887649568\n",
      "  batch 1000 loss: 9.067647929775902e-05\n",
      "  batch 1500 loss: 0.002129951876316305\n",
      "  batch 2000 loss: 0.0023911503004009765\n",
      "  batch 2500 loss: 0.00018916599181023486\n",
      "  batch 3000 loss: 0.02431241264070021\n",
      "  batch 3500 loss: 0.000701942290124304\n",
      "  batch 4000 loss: 0.0037281707185461456\n",
      "  batch 4500 loss: 0.0008803750187249904\n",
      "  batch 5000 loss: 0.00027954385493075053\n",
      "  batch 5500 loss: 0.0002150179440712776\n",
      "  batch 6000 loss: 0.0002739959732971862\n",
      "  batch 6500 loss: 0.005909262563761775\n",
      "  batch 7000 loss: 0.00034276098496860993\n",
      "  batch 7500 loss: 0.00045132805063457225\n",
      "  batch 8000 loss: 0.0002064358637739403\n",
      "  batch 8500 loss: 0.00012891102487456863\n",
      "  batch 9000 loss: 0.0023662377428926005\n",
      "  batch 9500 loss: 0.00016698526762398558\n",
      "  batch 10000 loss: 8.052698185316132e-05\n",
      "  batch 10500 loss: 0.00040052728581835593\n",
      "  batch 11000 loss: 0.00037412327964823076\n",
      "  batch 11500 loss: 0.0001419055179561681\n",
      "  batch 12000 loss: 0.0003376967335830514\n",
      "  batch 12500 loss: 0.00016102822124434724\n",
      "  batch 13000 loss: 0.0025804114003419834\n",
      "  batch 13500 loss: 0.0049529054183630305\n",
      "  batch 14000 loss: 0.0002577703726442273\n",
      "  batch 14500 loss: 0.0002515297582972345\n",
      "  batch 15000 loss: 9.775474795317508e-05\n",
      "  batch 15500 loss: 0.0006182915214276328\n",
      "  batch 16000 loss: 0.0007654811293401948\n",
      "  batch 16500 loss: 0.005353913288386259\n",
      "  batch 17000 loss: 0.0004643837741858086\n",
      "  batch 17500 loss: 0.002861763363060927\n",
      "  batch 18000 loss: 0.0032170199387039737\n",
      "  batch 18500 loss: 0.0002793027439441502\n",
      "  batch 19000 loss: 0.0009346966743585376\n",
      "  batch 19500 loss: 0.00020596409373667513\n",
      "  batch 20000 loss: 0.00033774620613467475\n",
      "  batch 20500 loss: 0.000775281684175539\n",
      "  batch 21000 loss: 0.0010140607035363373\n",
      "  batch 21500 loss: 0.00044161988263686423\n",
      "  batch 22000 loss: 0.000202487595280747\n",
      "  batch 22500 loss: 0.0004510108226461931\n",
      "  batch 23000 loss: 0.0007284842019913818\n",
      "  batch 23500 loss: 0.00027823498146520363\n",
      "  batch 24000 loss: 0.00016942009879521435\n",
      "  batch 24500 loss: 0.0001217486739469571\n",
      "  batch 25000 loss: 0.00035333311691046986\n",
      "  batch 25500 loss: 0.001270438745063892\n",
      "  batch 26000 loss: 0.00015367298723608158\n",
      "  batch 26500 loss: 0.00017932991814256384\n",
      "  batch 27000 loss: 0.00030514678424784945\n",
      "LOSS train 0.00030514678424784945 valid 0.0015737871320936534\n",
      "EPOCH 36:\n",
      "  batch 500 loss: 0.0003484312895272055\n",
      "  batch 1000 loss: 0.0001968662847811231\n",
      "  batch 1500 loss: 0.00012252396934670529\n",
      "  batch 2000 loss: 0.0016338676215152646\n",
      "  batch 2500 loss: 0.00032033471520646016\n",
      "  batch 3000 loss: 0.0007352218212765856\n",
      "  batch 3500 loss: 0.0004393203668175154\n",
      "  batch 4000 loss: 0.00022827421411168914\n",
      "  batch 4500 loss: 0.00025773907738596335\n",
      "  batch 5000 loss: 0.00040806102945399305\n",
      "  batch 5500 loss: 6.798814892306027e-05\n",
      "  batch 6000 loss: 0.003818912664535816\n",
      "  batch 6500 loss: 0.0001580816207432818\n",
      "  batch 7000 loss: 0.00035509435766883614\n",
      "  batch 7500 loss: 6.721921770763117e-05\n",
      "  batch 8000 loss: 0.0007823680942691099\n",
      "  batch 8500 loss: 0.00018317609028824577\n",
      "  batch 9000 loss: 7.496718208029307e-05\n",
      "  batch 9500 loss: 7.98102902667317e-05\n",
      "  batch 10000 loss: 8.818640289597468e-05\n",
      "  batch 10500 loss: 0.0031777645745287336\n",
      "  batch 11000 loss: 0.014491234791389975\n",
      "  batch 11500 loss: 0.0006092970188496451\n",
      "  batch 12000 loss: 0.0016638268673243707\n",
      "  batch 12500 loss: 0.005053481707967691\n",
      "  batch 13000 loss: 0.0005387127347020027\n",
      "  batch 13500 loss: 0.00015304416952213629\n",
      "  batch 14000 loss: 0.00048089043247198047\n",
      "  batch 14500 loss: 0.0005051844827817149\n",
      "  batch 15000 loss: 9.084758280273419e-05\n",
      "  batch 15500 loss: 0.00010881625719836308\n",
      "  batch 16000 loss: 8.340573208870694e-05\n",
      "  batch 16500 loss: 0.00017631208943244303\n",
      "  batch 17000 loss: 0.00043649760533482864\n",
      "  batch 17500 loss: 0.004106656713074251\n",
      "  batch 18000 loss: 0.0002109262849983047\n",
      "  batch 18500 loss: 0.0004059942653958579\n",
      "  batch 19000 loss: 0.00015369006213715152\n",
      "  batch 19500 loss: 0.0013207751576100755\n",
      "  batch 20000 loss: 0.007324977887828119\n",
      "  batch 20500 loss: 0.018104056605137035\n",
      "  batch 21000 loss: 0.003299250551429097\n",
      "  batch 21500 loss: 0.0006295309071804773\n",
      "  batch 22000 loss: 0.00020697648194144237\n",
      "  batch 22500 loss: 0.00021239099252775163\n",
      "  batch 23000 loss: 0.0008307635922901576\n",
      "  batch 23500 loss: 0.000379062027730189\n",
      "  batch 24000 loss: 0.0007857031426477299\n",
      "  batch 24500 loss: 0.005204223457898461\n",
      "  batch 25000 loss: 0.00013881375467644119\n",
      "  batch 25500 loss: 0.0010584518957127535\n",
      "  batch 26000 loss: 0.00013476451161961478\n",
      "  batch 26500 loss: 0.0001959343097089352\n",
      "  batch 27000 loss: 0.00046705869645021636\n",
      "LOSS train 0.00046705869645021636 valid 0.0012549263406915958\n",
      "EPOCH 37:\n",
      "  batch 500 loss: 0.00031449779954898817\n",
      "  batch 1000 loss: 9.827785806367828e-05\n",
      "  batch 1500 loss: 0.00016894467456636875\n",
      "  batch 2000 loss: 0.00039783753286920385\n",
      "  batch 2500 loss: 0.0013505498606186136\n",
      "  batch 3000 loss: 0.00012850932012149841\n",
      "  batch 3500 loss: 0.00023542537641851524\n",
      "  batch 4000 loss: 0.004275835591594209\n",
      "  batch 4500 loss: 0.00010672459267360068\n",
      "  batch 5000 loss: 0.0013384176386551517\n",
      "  batch 5500 loss: 0.00010583069221436148\n",
      "  batch 6000 loss: 0.0010129981018497567\n",
      "  batch 6500 loss: 0.00022154030266510104\n",
      "  batch 7000 loss: 8.089915134177517e-05\n",
      "  batch 7500 loss: 0.00028105869726840636\n",
      "  batch 8000 loss: 0.0002000659610081712\n",
      "  batch 8500 loss: 0.0002391768433861863\n",
      "  batch 9000 loss: 6.633978254061646e-05\n",
      "  batch 9500 loss: 7.17195715819372e-05\n",
      "  batch 10000 loss: 7.337269072211683e-05\n",
      "  batch 10500 loss: 0.0001748135427927622\n",
      "  batch 11000 loss: 0.0007252372121367827\n",
      "  batch 11500 loss: 9.602122225774324e-05\n",
      "  batch 12000 loss: 0.00011622773304866385\n",
      "  batch 12500 loss: 6.749078132175157e-05\n",
      "  batch 13000 loss: 0.003234008065505801\n",
      "  batch 13500 loss: 0.006005673996414689\n",
      "  batch 14000 loss: 0.00038916083812463143\n",
      "  batch 14500 loss: 0.0001139396119274636\n",
      "  batch 15000 loss: 0.0002994368345952232\n",
      "  batch 15500 loss: 0.0005899715222774695\n",
      "  batch 16000 loss: 0.0005606496358644862\n",
      "  batch 16500 loss: 8.527459493319256e-05\n",
      "  batch 17000 loss: 0.0029869669115913347\n",
      "  batch 17500 loss: 0.00030873511046160826\n",
      "  batch 18000 loss: 0.00014909839962597005\n",
      "  batch 18500 loss: 7.600787907218987e-05\n",
      "  batch 19000 loss: 0.0007004026195036097\n",
      "  batch 19500 loss: 0.0021487455723741035\n",
      "  batch 20000 loss: 0.00031089310809449697\n",
      "  batch 20500 loss: 9.793216895026546e-05\n",
      "  batch 21000 loss: 0.0001450821860917557\n",
      "  batch 21500 loss: 0.000423740234681091\n",
      "  batch 22000 loss: 0.0001114200410168813\n",
      "  batch 22500 loss: 0.0006124141833297117\n",
      "  batch 23000 loss: 0.0001500474339421878\n",
      "  batch 23500 loss: 0.0243916568859605\n",
      "  batch 24000 loss: 0.00982403062874222\n",
      "  batch 24500 loss: 0.0025011066214957545\n",
      "  batch 25000 loss: 0.0016682252356544005\n",
      "  batch 25500 loss: 0.000312961956044866\n",
      "  batch 26000 loss: 0.00013549590545994405\n",
      "  batch 26500 loss: 0.0003161811608506788\n",
      "  batch 27000 loss: 0.00012523501691746476\n",
      "LOSS train 0.00012523501691746476 valid 0.001418499012674608\n",
      "EPOCH 38:\n",
      "  batch 500 loss: 0.0001238749747368537\n",
      "  batch 1000 loss: 0.00033759791347337753\n",
      "  batch 1500 loss: 0.0005456856502408626\n",
      "  batch 2000 loss: 0.00010928877344859345\n",
      "  batch 2500 loss: 0.001120600093718977\n",
      "  batch 3000 loss: 0.00021508898623656236\n",
      "  batch 3500 loss: 0.00020792641994228232\n",
      "  batch 4000 loss: 0.0001590085209364176\n",
      "  batch 4500 loss: 7.563530205361601e-05\n",
      "  batch 5000 loss: 0.00015088840151291906\n",
      "  batch 5500 loss: 6.0157242602169704e-05\n",
      "  batch 6000 loss: 0.00036821414667525333\n",
      "  batch 6500 loss: 0.00012557662857006592\n",
      "  batch 7000 loss: 9.499404406150801e-05\n",
      "  batch 7500 loss: 0.001670544854523456\n",
      "  batch 8000 loss: 0.0002475114565540899\n",
      "  batch 8500 loss: 0.00014412734209721733\n",
      "  batch 9000 loss: 0.00016628784796710505\n",
      "  batch 9500 loss: 0.00022544947417244286\n",
      "  batch 10000 loss: 0.00456266594077977\n",
      "  batch 10500 loss: 0.00040109600322371365\n",
      "  batch 11000 loss: 0.000258571961831759\n",
      "  batch 11500 loss: 0.00010661119730777812\n",
      "  batch 12000 loss: 9.917717758167654e-05\n",
      "  batch 12500 loss: 0.004662705898857087\n",
      "  batch 13000 loss: 7.529125709739049e-05\n",
      "  batch 13500 loss: 0.00010927168356703732\n",
      "  batch 14000 loss: 0.00030125854381854824\n",
      "  batch 14500 loss: 0.0006085485407860354\n",
      "  batch 15000 loss: 0.0019315825410455823\n",
      "  batch 15500 loss: 0.0001491097044322025\n",
      "  batch 16000 loss: 0.00047745384680914426\n",
      "  batch 16500 loss: 8.398400379549642e-05\n",
      "  batch 17000 loss: 0.000995576986350386\n",
      "  batch 17500 loss: 0.0003530070377731285\n",
      "  batch 18000 loss: 0.0003316757331153717\n",
      "  batch 18500 loss: 0.00032279503447045244\n",
      "  batch 19000 loss: 0.00014389445070407447\n",
      "  batch 19500 loss: 0.00017850248745402198\n",
      "  batch 20000 loss: 0.00017874952632859832\n",
      "  batch 20500 loss: 0.00010975020823363835\n",
      "  batch 21000 loss: 0.0004915557198227347\n",
      "  batch 21500 loss: 0.00369995598682236\n",
      "  batch 22000 loss: 0.0008254001774591231\n",
      "  batch 22500 loss: 0.0013676684563822263\n",
      "  batch 23000 loss: 0.0010103080421251605\n",
      "  batch 23500 loss: 0.030786045018857244\n",
      "  batch 24000 loss: 0.02765143409549165\n",
      "  batch 24500 loss: 0.041864733195816344\n",
      "  batch 25000 loss: 0.008877014436682632\n",
      "  batch 25500 loss: 0.002781702733702126\n",
      "  batch 26000 loss: 0.0006091328713925819\n",
      "  batch 26500 loss: 0.00032338897345812257\n",
      "  batch 27000 loss: 0.00018737347597258137\n",
      "LOSS train 0.00018737347597258137 valid 0.09900681606888748\n",
      "EPOCH 39:\n",
      "  batch 500 loss: 0.005170991299848616\n",
      "  batch 1000 loss: 0.0006414574905138792\n",
      "  batch 1500 loss: 0.00033458716548596354\n",
      "  batch 2000 loss: 0.0033786859135190427\n",
      "  batch 2500 loss: 0.00022128332955227137\n",
      "  batch 3000 loss: 0.004443237710245303\n",
      "  batch 3500 loss: 0.0001874923439663476\n",
      "  batch 4000 loss: 0.0003379375734719687\n",
      "  batch 4500 loss: 0.0005482955777249465\n",
      "  batch 5000 loss: 0.0002570777737187271\n",
      "  batch 5500 loss: 0.0002586305195969949\n",
      "  batch 6000 loss: 0.00021130019899886677\n",
      "  batch 6500 loss: 0.001448604140819775\n",
      "  batch 7000 loss: 0.014355511407726273\n",
      "  batch 7500 loss: 0.00014741485406388933\n",
      "  batch 8000 loss: 0.00010030181504823687\n",
      "  batch 8500 loss: 0.003376999102813727\n",
      "  batch 9000 loss: 0.002717770007527207\n",
      "  batch 9500 loss: 0.001701994683426914\n",
      "  batch 10000 loss: 0.014476599668462828\n",
      "  batch 10500 loss: 0.012980849926022244\n",
      "  batch 11000 loss: 0.012883885678209382\n",
      "  batch 11500 loss: 0.022093301965088692\n",
      "  batch 12000 loss: 0.0012564779739279715\n",
      "  batch 12500 loss: 0.0002858372120720105\n",
      "  batch 13000 loss: 0.013026940782205827\n",
      "  batch 13500 loss: 0.0012913484037969454\n",
      "  batch 14000 loss: 0.00016726157487136106\n",
      "  batch 14500 loss: 0.00022966536514848812\n",
      "  batch 15000 loss: 0.0015565071448760683\n",
      "  batch 15500 loss: 0.000625777474838955\n",
      "  batch 16000 loss: 0.00019376336304216223\n",
      "  batch 16500 loss: 0.0017167939427983861\n",
      "  batch 17000 loss: 0.006195755211665798\n",
      "  batch 17500 loss: 0.0012685659683688293\n",
      "  batch 18000 loss: 0.0005097941721128265\n",
      "  batch 18500 loss: 0.0006763689399405628\n",
      "  batch 19000 loss: 0.00011377059352199837\n",
      "  batch 19500 loss: 0.00014968596282120573\n",
      "  batch 20000 loss: 0.00021628127875399273\n",
      "  batch 20500 loss: 0.00014535683958363777\n",
      "  batch 21000 loss: 0.00013134554460139469\n",
      "  batch 21500 loss: 0.00014357942500302912\n",
      "  batch 22000 loss: 0.00044464196232555866\n",
      "  batch 22500 loss: 0.00018097010557790937\n",
      "  batch 23000 loss: 0.004449262049363362\n",
      "  batch 23500 loss: 0.0004153214658981952\n",
      "  batch 24000 loss: 0.000170578531635023\n",
      "  batch 24500 loss: 0.00012610255742762178\n",
      "  batch 25000 loss: 0.00010455915903029833\n",
      "  batch 25500 loss: 0.004225465795140209\n",
      "  batch 26000 loss: 0.011479374339004672\n",
      "  batch 26500 loss: 0.008712426399749977\n",
      "  batch 27000 loss: 0.00040794058564542726\n",
      "LOSS train 0.00040794058564542726 valid 0.0014088047409057287\n",
      "EPOCH 40:\n",
      "  batch 500 loss: 0.00011711902512470829\n",
      "  batch 1000 loss: 0.00021373935545126698\n",
      "  batch 1500 loss: 0.00011523979978645116\n",
      "  batch 2000 loss: 0.0009002472700112918\n",
      "  batch 2500 loss: 0.0004103083303044492\n",
      "  batch 3000 loss: 0.00030674687823248445\n",
      "  batch 3500 loss: 8.907026937387031e-05\n",
      "  batch 4000 loss: 0.000753710971368907\n",
      "  batch 4500 loss: 0.00017555142408582115\n",
      "  batch 5000 loss: 0.00018458055235535652\n",
      "  batch 5500 loss: 7.74254565004604e-05\n",
      "  batch 6000 loss: 0.0006171814530414572\n",
      "  batch 6500 loss: 0.0001326580245218203\n",
      "  batch 7000 loss: 0.0004914297323254928\n",
      "  batch 7500 loss: 0.0006023547754328007\n",
      "  batch 8000 loss: 0.0008963266428348255\n",
      "  batch 8500 loss: 0.00021033252949062485\n",
      "  batch 9000 loss: 0.0007819773029222574\n",
      "  batch 9500 loss: 0.043782758124947946\n",
      "  batch 10000 loss: 0.0040397063274958496\n",
      "  batch 10500 loss: 0.000783251877648322\n",
      "  batch 11000 loss: 0.0003582166458779952\n",
      "  batch 11500 loss: 0.0002727459670090724\n",
      "  batch 12000 loss: 0.00023217807172846606\n",
      "  batch 12500 loss: 0.0004754386279343876\n",
      "  batch 13000 loss: 0.00015913153784757485\n",
      "  batch 13500 loss: 0.00010546956752574488\n",
      "  batch 14000 loss: 6.220351832503468e-05\n",
      "  batch 14500 loss: 0.0001261334162012062\n",
      "  batch 15000 loss: 0.0008684871571174817\n",
      "  batch 15500 loss: 0.0012727009060275875\n",
      "  batch 16000 loss: 0.00026502258930895906\n",
      "  batch 16500 loss: 0.0009424904445090583\n",
      "  batch 17000 loss: 7.107931746714158e-05\n",
      "  batch 17500 loss: 0.001901954494993653\n",
      "  batch 18000 loss: 7.833003373560387e-05\n",
      "  batch 18500 loss: 0.00013964469571392258\n",
      "  batch 19000 loss: 0.00011101109752870286\n",
      "  batch 19500 loss: 0.00460503720578701\n",
      "  batch 20000 loss: 0.00028348072719849425\n",
      "  batch 20500 loss: 0.000116232986305274\n",
      "  batch 21000 loss: 0.0015722310494016441\n",
      "  batch 21500 loss: 7.917511668883036e-05\n",
      "  batch 22000 loss: 7.98748489504355e-05\n",
      "  batch 22500 loss: 0.0018244554086324066\n",
      "  batch 23000 loss: 0.0001186867459637071\n",
      "  batch 23500 loss: 0.0008585628448339442\n",
      "  batch 24000 loss: 0.0011618472979209144\n",
      "  batch 24500 loss: 0.00016966122566796572\n",
      "  batch 25000 loss: 0.00037294265707878436\n",
      "  batch 25500 loss: 0.00022546443302680076\n",
      "  batch 26000 loss: 0.00011166466247806994\n",
      "  batch 26500 loss: 0.0005498185655818801\n",
      "  batch 27000 loss: 0.004716349510413973\n",
      "LOSS train 0.004716349510413973 valid 0.001174400821716499\n",
      "EPOCH 41:\n",
      "  batch 500 loss: 0.00014092970022860384\n",
      "  batch 1000 loss: 0.00026725482304776804\n",
      "  batch 1500 loss: 0.00022284668735423452\n",
      "  batch 2000 loss: 0.00016077760261397244\n",
      "  batch 2500 loss: 0.0002981811957038509\n",
      "  batch 3000 loss: 0.00017222592379019375\n",
      "  batch 3500 loss: 0.00012046277274581741\n",
      "  batch 4000 loss: 9.520686459250527e-05\n",
      "  batch 4500 loss: 0.009213664533935866\n",
      "  batch 5000 loss: 0.0004667473650625702\n",
      "  batch 5500 loss: 0.0031351621315706597\n",
      "  batch 6000 loss: 0.004780012704767902\n",
      "  batch 6500 loss: 0.0004096594073882116\n",
      "  batch 7000 loss: 0.0006455624707799999\n",
      "  batch 7500 loss: 0.0016297746590092609\n",
      "  batch 8000 loss: 0.00017853363399181532\n",
      "  batch 8500 loss: 0.00018197779567670125\n",
      "  batch 9000 loss: 0.00029955076559107143\n",
      "  batch 9500 loss: 0.00031330182302868084\n",
      "  batch 10000 loss: 0.00014556903044466907\n",
      "  batch 10500 loss: 0.00019982956934553543\n",
      "  batch 11000 loss: 6.513256603970419e-05\n",
      "  batch 11500 loss: 6.108042660082447e-05\n",
      "  batch 12000 loss: 0.00036222937890576645\n",
      "  batch 12500 loss: 0.000721016448363212\n",
      "  batch 13000 loss: 0.00014609234302495367\n",
      "  batch 13500 loss: 8.551472145675376e-05\n",
      "  batch 14000 loss: 0.0008331924773503587\n",
      "  batch 14500 loss: 0.0011943994507091383\n",
      "  batch 15000 loss: 0.0002579603352628723\n",
      "  batch 15500 loss: 0.0005451226865660246\n",
      "  batch 16000 loss: 7.556021882565744e-05\n",
      "  batch 16500 loss: 0.00017534321097813788\n",
      "  batch 17000 loss: 0.000132811669991046\n",
      "  batch 17500 loss: 0.000312375287668754\n",
      "  batch 18000 loss: 7.343780212579532e-05\n",
      "  batch 18500 loss: 0.00016052545896075543\n",
      "  batch 19000 loss: 0.00011390731950253396\n",
      "  batch 19500 loss: 0.00013480889239327\n",
      "  batch 20000 loss: 0.00010469812652303645\n",
      "  batch 20500 loss: 0.00012219807500599345\n",
      "  batch 21000 loss: 7.551370742080365e-05\n",
      "  batch 21500 loss: 0.0007155155145709848\n",
      "  batch 22000 loss: 0.00018329212263276418\n",
      "  batch 22500 loss: 9.317155362080243e-05\n",
      "  batch 23000 loss: 5.869063639423899e-05\n",
      "  batch 23500 loss: 7.80621389241638e-05\n",
      "  batch 24000 loss: 0.0004380365568299851\n",
      "  batch 24500 loss: 0.00038607909645813266\n",
      "  batch 25000 loss: 0.00015652226080857545\n",
      "  batch 25500 loss: 0.00017259743877496802\n",
      "  batch 26000 loss: 0.00021285478262812063\n",
      "  batch 26500 loss: 0.00015287453185352718\n",
      "  batch 27000 loss: 0.006334686109572487\n",
      "LOSS train 0.006334686109572487 valid 0.002542915013317241\n",
      "EPOCH 42:\n",
      "  batch 500 loss: 0.000231735760493077\n",
      "  batch 1000 loss: 7.05574406796714e-05\n",
      "  batch 1500 loss: 0.0003217112326993501\n",
      "  batch 2000 loss: 0.00014077527002284284\n",
      "  batch 2500 loss: 0.00014808631997087574\n",
      "  batch 3000 loss: 0.001924053835566653\n",
      "  batch 3500 loss: 0.0005672637778780718\n",
      "  batch 4000 loss: 0.0014419240871219366\n",
      "  batch 4500 loss: 0.01954944551836513\n",
      "  batch 5000 loss: 0.0005414718900664326\n",
      "  batch 5500 loss: 0.0001834954126092505\n",
      "  batch 6000 loss: 0.001695081901105926\n",
      "  batch 6500 loss: 0.00022294994511272748\n",
      "  batch 7000 loss: 0.00024417665187674944\n",
      "  batch 7500 loss: 0.0002929343861815852\n",
      "  batch 8000 loss: 0.0004044488230151373\n",
      "  batch 8500 loss: 0.0003561204645723457\n",
      "  batch 9000 loss: 0.00012987991586662417\n",
      "  batch 9500 loss: 0.000131111541753814\n",
      "  batch 10000 loss: 0.0002583291809936519\n",
      "  batch 10500 loss: 0.0001378897422643277\n",
      "  batch 11000 loss: 5.6369484009607616e-05\n",
      "  batch 11500 loss: 0.0015438143813758956\n",
      "  batch 12000 loss: 0.0012225037140209522\n",
      "  batch 12500 loss: 9.536343329510543e-05\n",
      "  batch 13000 loss: 7.211982884091129e-05\n",
      "  batch 13500 loss: 0.0008671493253069364\n",
      "  batch 14000 loss: 0.0035355470467294944\n",
      "  batch 14500 loss: 0.0001747956201289469\n",
      "  batch 15000 loss: 0.0012143315819022683\n",
      "  batch 15500 loss: 0.0012423746630626624\n",
      "  batch 16000 loss: 0.0002427757011789744\n",
      "  batch 16500 loss: 0.03164303011400336\n",
      "  batch 17000 loss: 0.0004336746048349127\n",
      "  batch 17500 loss: 0.00016289406800075668\n",
      "  batch 18000 loss: 0.000254078347258762\n",
      "  batch 18500 loss: 0.00024234088092371309\n",
      "  batch 19000 loss: 9.729702428249354e-05\n",
      "  batch 19500 loss: 0.00017269021600882793\n",
      "  batch 20000 loss: 9.590912655664141e-05\n",
      "  batch 20500 loss: 9.021824302791615e-05\n",
      "  batch 21000 loss: 0.005092221308543113\n",
      "  batch 21500 loss: 0.001243770283992024\n",
      "  batch 22000 loss: 0.005336021417901851\n",
      "  batch 22500 loss: 0.004891486987039148\n",
      "  batch 23000 loss: 0.0005353996898317454\n",
      "  batch 23500 loss: 0.0003101195905309595\n",
      "  batch 24000 loss: 0.0002851823426876052\n",
      "  batch 24500 loss: 7.631902783874268e-05\n",
      "  batch 25000 loss: 0.00016573161978156392\n",
      "  batch 25500 loss: 0.0004007862773439577\n",
      "  batch 26000 loss: 0.00033074371659980173\n",
      "  batch 26500 loss: 0.0008062280258781946\n",
      "  batch 27000 loss: 0.00020800848033994512\n",
      "LOSS train 0.00020800848033994512 valid 0.001108323466920145\n",
      "EPOCH 43:\n",
      "  batch 500 loss: 0.00041710164240025806\n",
      "  batch 1000 loss: 0.0001043617994948285\n",
      "  batch 1500 loss: 0.000299219582334068\n",
      "  batch 2000 loss: 0.00011755029576315579\n",
      "  batch 2500 loss: 0.00011990239634269883\n",
      "  batch 3000 loss: 0.003171765868822224\n",
      "  batch 3500 loss: 0.00011140950521550508\n",
      "  batch 4000 loss: 0.00021593558026756555\n",
      "  batch 4500 loss: 0.00011018041280183866\n",
      "  batch 5000 loss: 0.00029071105645745376\n",
      "  batch 5500 loss: 0.00016176500327117792\n",
      "  batch 6000 loss: 0.0003129268062404087\n",
      "  batch 6500 loss: 0.00011077942265803031\n",
      "  batch 7000 loss: 0.002434471746799751\n",
      "  batch 7500 loss: 0.0024175318009421218\n",
      "  batch 8000 loss: 0.00030104840520546006\n",
      "  batch 8500 loss: 0.00018238037826459675\n",
      "  batch 9000 loss: 0.00010642120878056715\n",
      "  batch 9500 loss: 8.832347576054644e-05\n",
      "  batch 10000 loss: 0.00015458192186566677\n",
      "  batch 10500 loss: 8.471853243100824e-05\n",
      "  batch 11000 loss: 0.0001683467944899597\n",
      "  batch 11500 loss: 0.00015188796012044036\n",
      "  batch 12000 loss: 0.0004776267499330729\n",
      "  batch 12500 loss: 0.0002642639128787998\n",
      "  batch 13000 loss: 0.00015845375970077667\n",
      "  batch 13500 loss: 0.00013118971317081928\n",
      "  batch 14000 loss: 0.0005825205348929358\n",
      "  batch 14500 loss: 0.0019092104573576946\n",
      "  batch 15000 loss: 0.00033780572534030284\n",
      "  batch 15500 loss: 8.567850765436092e-05\n",
      "  batch 16000 loss: 0.007402874091752839\n",
      "  batch 16500 loss: 0.0001360267881349806\n",
      "  batch 17000 loss: 0.00012685770828339927\n",
      "  batch 17500 loss: 0.0002058118609912789\n",
      "  batch 18000 loss: 0.002004896985322695\n",
      "  batch 18500 loss: 0.0004033560308278865\n",
      "  batch 19000 loss: 8.413607235380738e-05\n",
      "  batch 19500 loss: 0.00015469854293225893\n",
      "  batch 20000 loss: 0.00023402091423988834\n",
      "  batch 20500 loss: 0.00029620446188156533\n",
      "  batch 21000 loss: 0.00011271195868388516\n",
      "  batch 21500 loss: 5.5869151651066315e-05\n",
      "  batch 22000 loss: 0.000246214394449229\n",
      "  batch 22500 loss: 0.00010267936189277548\n",
      "  batch 23000 loss: 4.815851221265177e-05\n",
      "  batch 23500 loss: 7.440151317402765e-05\n",
      "  batch 24000 loss: 0.0012686251313582346\n",
      "  batch 24500 loss: 0.00026900539486658914\n",
      "  batch 25000 loss: 0.00010690910550490073\n",
      "  batch 25500 loss: 7.058947273623063e-05\n",
      "  batch 26000 loss: 0.00020151122126337029\n",
      "  batch 26500 loss: 0.0004490535863343865\n",
      "  batch 27000 loss: 0.0003882345278919601\n",
      "LOSS train 0.0003882345278919601 valid 0.0010854271383945462\n",
      "EPOCH 44:\n",
      "  batch 500 loss: 0.00014250719676296697\n",
      "  batch 1000 loss: 4.292889487340545e-05\n",
      "  batch 1500 loss: 0.00018313403477323575\n",
      "  batch 2000 loss: 9.256205069895884e-05\n",
      "  batch 2500 loss: 0.00013956915274144778\n",
      "  batch 3000 loss: 9.352448058893614e-05\n",
      "  batch 3500 loss: 4.709604315473825e-05\n",
      "  batch 4000 loss: 0.00011930719127130374\n",
      "  batch 4500 loss: 0.0014809387303182469\n",
      "  batch 5000 loss: 0.0002798023588305476\n",
      "  batch 5500 loss: 0.00012012130163734724\n",
      "  batch 6000 loss: 0.00019868818981628067\n",
      "  batch 6500 loss: 0.00020778322559502626\n",
      "  batch 7000 loss: 7.765068980990008e-05\n",
      "  batch 7500 loss: 0.00023294540747297886\n",
      "  batch 8000 loss: 0.00023231948213384257\n",
      "  batch 8500 loss: 0.00011282150893991628\n",
      "  batch 9000 loss: 9.242386328727647e-05\n",
      "  batch 9500 loss: 5.7793586939002494e-05\n",
      "  batch 10000 loss: 5.3929243173506334e-05\n",
      "  batch 10500 loss: 0.0001267738836617589\n",
      "  batch 11000 loss: 0.00030233701759741296\n",
      "  batch 11500 loss: 9.473991039271823e-05\n",
      "  batch 12000 loss: 0.0002680569039846077\n",
      "  batch 12500 loss: 6.851371397718964e-05\n",
      "  batch 13000 loss: 0.00012838456936877662\n",
      "  batch 13500 loss: 0.00021749404410100937\n",
      "  batch 14000 loss: 5.683306687473077e-05\n",
      "  batch 14500 loss: 4.822247254899992e-05\n",
      "  batch 15000 loss: 6.057990569941652e-05\n",
      "  batch 15500 loss: 0.004076793478318222\n",
      "  batch 16000 loss: 0.00014807511107860804\n",
      "  batch 16500 loss: 8.799067273980655e-05\n",
      "  batch 17000 loss: 3.941732119116637e-05\n",
      "  batch 17500 loss: 7.054195696079547e-05\n",
      "  batch 18000 loss: 0.0004407745701444519\n",
      "  batch 18500 loss: 0.0035857895135404123\n",
      "  batch 19000 loss: 0.00036570930993994467\n",
      "  batch 19500 loss: 0.003831842717649547\n",
      "  batch 20000 loss: 6.87261236681742e-05\n",
      "  batch 20500 loss: 0.00018010590027535756\n",
      "  batch 21000 loss: 5.840061010276187e-05\n",
      "  batch 21500 loss: 7.452335182405179e-05\n",
      "  batch 22000 loss: 0.00014590575412448814\n",
      "  batch 22500 loss: 8.8903819064317e-05\n",
      "  batch 23000 loss: 6.35304098088767e-05\n",
      "  batch 23500 loss: 0.001724872052313163\n",
      "  batch 24000 loss: 9.258597900869603e-05\n",
      "  batch 24500 loss: 0.0004386752469917603\n",
      "  batch 25000 loss: 0.0001543751383514831\n",
      "  batch 25500 loss: 0.00013880052872485392\n",
      "  batch 26000 loss: 9.499842631268862e-05\n",
      "  batch 26500 loss: 0.0018866548263574536\n",
      "  batch 27000 loss: 0.00021969381645927654\n",
      "LOSS train 0.00021969381645927654 valid 0.001431196124083822\n",
      "EPOCH 45:\n",
      "  batch 500 loss: 0.005342501044862441\n",
      "  batch 1000 loss: 0.0009992305372390966\n",
      "  batch 1500 loss: 0.00021031023297335237\n",
      "  batch 2000 loss: 0.0001802805488955528\n",
      "  batch 2500 loss: 0.0007177911250534344\n",
      "  batch 3000 loss: 0.002845227463028518\n",
      "  batch 3500 loss: 0.00022968447101287382\n",
      "  batch 4000 loss: 0.00022024528640447372\n",
      "  batch 4500 loss: 0.00011368997311076967\n",
      "  batch 5000 loss: 0.00573388402613785\n",
      "  batch 5500 loss: 0.00010063272995510886\n",
      "  batch 6000 loss: 0.0016946140011866256\n",
      "  batch 6500 loss: 0.00027311946884566395\n",
      "  batch 7000 loss: 0.00010822309806693298\n",
      "  batch 7500 loss: 6.471933192731428e-05\n",
      "  batch 8000 loss: 0.0003210385992452807\n",
      "  batch 8500 loss: 0.00039760071827661745\n",
      "  batch 9000 loss: 0.00011968001020182229\n",
      "  batch 9500 loss: 0.0003552818416103385\n",
      "  batch 10000 loss: 0.0003521633419824326\n",
      "  batch 10500 loss: 8.803092170438732e-05\n",
      "  batch 11000 loss: 5.5134063545811074e-05\n",
      "  batch 11500 loss: 6.117046560018125e-05\n",
      "  batch 12000 loss: 4.9302768969379685e-05\n",
      "  batch 12500 loss: 4.4478973821764354e-05\n",
      "  batch 13000 loss: 9.600364546522044e-05\n",
      "  batch 13500 loss: 8.970549292926222e-05\n",
      "  batch 14000 loss: 7.32229238093538e-05\n",
      "  batch 14500 loss: 5.3312172388402246e-05\n",
      "  batch 15000 loss: 0.0003095244931837939\n",
      "  batch 15500 loss: 0.00015519814903160168\n",
      "  batch 16000 loss: 4.867467289812666e-05\n",
      "  batch 16500 loss: 8.00546543109668e-05\n",
      "  batch 17000 loss: 0.00011174100434373457\n",
      "  batch 17500 loss: 6.474464538250047e-05\n",
      "  batch 18000 loss: 7.275743024211323e-05\n",
      "  batch 18500 loss: 0.0003049329182062408\n",
      "  batch 19000 loss: 0.0001263033725304048\n",
      "  batch 19500 loss: 0.00011022404680312192\n",
      "  batch 20000 loss: 0.00010617109320991602\n",
      "  batch 20500 loss: 5.0234799974514746e-05\n",
      "  batch 21000 loss: 5.192001959519388e-05\n",
      "  batch 21500 loss: 6.407805601856964e-05\n",
      "  batch 22000 loss: 3.654293953929866e-05\n",
      "  batch 22500 loss: 0.00010139718177712354\n",
      "  batch 23000 loss: 0.0007006994931798544\n",
      "  batch 23500 loss: 0.00021139808780500503\n",
      "  batch 24000 loss: 8.21427681855802e-05\n",
      "  batch 24500 loss: 8.158544059999429e-05\n",
      "  batch 25000 loss: 0.0008724499376682538\n",
      "  batch 25500 loss: 0.004989961325044295\n",
      "  batch 26000 loss: 0.0014303550364183231\n",
      "  batch 26500 loss: 0.00012491625660321758\n",
      "  batch 27000 loss: 0.0010019322594465266\n",
      "LOSS train 0.0010019322594465266 valid 0.0032708971204828295\n",
      "EPOCH 46:\n",
      "  batch 500 loss: 0.000705346536463761\n",
      "  batch 1000 loss: 0.0003227701510827288\n",
      "  batch 1500 loss: 0.0011272247176047153\n",
      "  batch 2000 loss: 0.0001659309211018467\n",
      "  batch 2500 loss: 9.608933258816421e-05\n",
      "  batch 3000 loss: 5.034325512789195e-05\n",
      "  batch 3500 loss: 8.644920579274284e-05\n",
      "  batch 4000 loss: 0.00022219498585324615\n",
      "  batch 4500 loss: 0.01177391987095993\n",
      "  batch 5000 loss: 0.0037938765680025632\n",
      "  batch 5500 loss: 0.0004473141981761799\n",
      "  batch 6000 loss: 0.00020926851055659413\n",
      "  batch 6500 loss: 0.0006519172752231591\n",
      "  batch 7000 loss: 0.004126527610454296\n",
      "  batch 7500 loss: 0.00022454469891342298\n",
      "  batch 8000 loss: 0.00018009061812022153\n",
      "  batch 8500 loss: 6.861019524887979e-05\n",
      "  batch 9000 loss: 0.011866235786718452\n",
      "  batch 9500 loss: 0.00019496265547752146\n",
      "  batch 10000 loss: 7.186413244679812e-05\n",
      "  batch 10500 loss: 0.004903386121305331\n",
      "  batch 11000 loss: 0.00017379938173978714\n",
      "  batch 11500 loss: 0.00012152995637740106\n",
      "  batch 12000 loss: 0.0003467873951745517\n",
      "  batch 12500 loss: 0.0002908433119382252\n",
      "  batch 13000 loss: 0.00012907640947110722\n",
      "  batch 13500 loss: 0.00011267305926114446\n",
      "  batch 14000 loss: 0.00011499552610977659\n",
      "  batch 14500 loss: 0.00019573729669991025\n",
      "  batch 15000 loss: 0.0004830976817400057\n",
      "  batch 15500 loss: 0.0003911946578991028\n",
      "  batch 16000 loss: 0.0002775739683893086\n",
      "  batch 16500 loss: 0.002093377910051213\n",
      "  batch 17000 loss: 0.0002658274781203502\n",
      "  batch 17500 loss: 0.0001138742108463866\n",
      "  batch 18000 loss: 0.02513339432843292\n",
      "  batch 18500 loss: 0.00020329296187693657\n",
      "  batch 19000 loss: 0.00010572842657104787\n",
      "  batch 19500 loss: 0.0002783252699988168\n",
      "  batch 20000 loss: 5.448246120633016e-05\n",
      "  batch 20500 loss: 0.0003450592392868188\n",
      "  batch 21000 loss: 0.0002112194108636345\n",
      "  batch 21500 loss: 0.00019876481599464667\n",
      "  batch 22000 loss: 5.0500551892621814e-05\n",
      "  batch 22500 loss: 0.0001803843823874054\n",
      "  batch 23000 loss: 0.0006499489342324658\n",
      "  batch 23500 loss: 0.00016874741006243354\n",
      "  batch 24000 loss: 8.44291226277214e-05\n",
      "  batch 24500 loss: 0.00013838398396084984\n",
      "  batch 25000 loss: 6.76245682442449e-05\n",
      "  batch 25500 loss: 0.00013906384235323088\n",
      "  batch 26000 loss: 0.00016447218311866152\n",
      "  batch 26500 loss: 0.0005588274981766474\n",
      "  batch 27000 loss: 0.0011497210237315762\n",
      "LOSS train 0.0011497210237315762 valid 0.001335954759652348\n",
      "EPOCH 47:\n",
      "  batch 500 loss: 0.00015006761134573666\n",
      "  batch 1000 loss: 6.77113484937486e-05\n",
      "  batch 1500 loss: 9.380963912320794e-05\n",
      "  batch 2000 loss: 0.0007463920693848038\n",
      "  batch 2500 loss: 0.0001126698425157997\n",
      "  batch 3000 loss: 7.5711104861373e-05\n",
      "  batch 3500 loss: 0.0030135003121168928\n",
      "  batch 4000 loss: 0.0001536633450736531\n",
      "  batch 4500 loss: 0.00012159134468435796\n",
      "  batch 5000 loss: 6.708295858408065e-05\n",
      "  batch 5500 loss: 8.426645650734699e-05\n",
      "  batch 6000 loss: 8.647107366451934e-05\n",
      "  batch 6500 loss: 0.002091193259368936\n",
      "  batch 7000 loss: 0.0009093911596588526\n",
      "  batch 7500 loss: 6.578366706793659e-05\n",
      "  batch 8000 loss: 0.001482857248168969\n",
      "  batch 8500 loss: 0.0035907747443700124\n",
      "  batch 9000 loss: 9.47988984240844e-05\n",
      "  batch 9500 loss: 0.00021668559474809256\n",
      "  batch 10000 loss: 0.0001429365736717365\n",
      "  batch 10500 loss: 0.0002540027182947178\n",
      "  batch 11000 loss: 0.00016188935580633057\n",
      "  batch 11500 loss: 0.0006464237661951273\n",
      "  batch 12000 loss: 0.0016426204300417524\n",
      "  batch 12500 loss: 0.0004454958707415635\n",
      "  batch 13000 loss: 0.00029133608809466693\n",
      "  batch 13500 loss: 5.040047700779482e-05\n",
      "  batch 14000 loss: 0.00014643373945650496\n",
      "  batch 14500 loss: 0.0001913998472236429\n",
      "  batch 15000 loss: 4.791336854342276e-05\n",
      "  batch 15500 loss: 3.5222432213277696e-05\n",
      "  batch 16000 loss: 0.0001367201738868431\n",
      "  batch 16500 loss: 5.483602999543891e-05\n",
      "  batch 17000 loss: 5.5648201164395064e-05\n",
      "  batch 17500 loss: 0.00025078103002319454\n",
      "  batch 18000 loss: 0.00012242385728171\n",
      "  batch 18500 loss: 0.00018972904497594457\n",
      "  batch 19000 loss: 5.175854816446801e-05\n",
      "  batch 19500 loss: 8.434094771993372e-05\n",
      "  batch 20000 loss: 4.554803530366769e-05\n",
      "  batch 20500 loss: 0.012621758872093923\n",
      "  batch 21000 loss: 0.000536024971783668\n",
      "  batch 21500 loss: 0.00030089007613090144\n",
      "  batch 22000 loss: 0.00019985018888555217\n",
      "  batch 22500 loss: 0.000507552980739991\n",
      "  batch 23000 loss: 0.0012262524133027739\n",
      "  batch 23500 loss: 7.424079966614272e-05\n",
      "  batch 24000 loss: 4.9939369753406737e-05\n",
      "  batch 24500 loss: 0.00014421969643397858\n",
      "  batch 25000 loss: 7.34786770092697e-05\n",
      "  batch 25500 loss: 5.4412879118956424e-05\n",
      "  batch 26000 loss: 3.734115389772441e-05\n",
      "  batch 26500 loss: 4.513881060365321e-05\n",
      "  batch 27000 loss: 0.00014339805660348404\n",
      "LOSS train 0.00014339805660348404 valid 0.0013364806424901524\n",
      "EPOCH 48:\n",
      "  batch 500 loss: 0.00044032940317328696\n",
      "  batch 1000 loss: 0.00016264557364675268\n",
      "  batch 1500 loss: 6.153794593717876e-05\n",
      "  batch 2000 loss: 0.00032018116687101993\n",
      "  batch 2500 loss: 8.914423023088247e-05\n",
      "  batch 3000 loss: 5.1460532625881685e-05\n",
      "  batch 3500 loss: 6.642286708718358e-05\n",
      "  batch 4000 loss: 5.218402927693333e-05\n",
      "  batch 4500 loss: 0.00227894302049701\n",
      "  batch 5000 loss: 6.928709641229957e-05\n",
      "  batch 5500 loss: 0.000296292718411987\n",
      "  batch 6000 loss: 7.174230611879296e-05\n",
      "  batch 6500 loss: 0.00010866041645850899\n",
      "  batch 7000 loss: 4.687069478853445e-05\n",
      "  batch 7500 loss: 0.00065852260830518\n",
      "  batch 8000 loss: 8.580201344843275e-05\n",
      "  batch 8500 loss: 0.00021775084856262694\n",
      "  batch 9000 loss: 0.00028539084942455874\n",
      "  batch 9500 loss: 0.00023715343100326792\n",
      "  batch 10000 loss: 0.00016561026005281222\n",
      "  batch 10500 loss: 0.00017917314236624192\n",
      "  batch 11000 loss: 7.957482627368151e-05\n",
      "  batch 11500 loss: 4.7627203190113935e-05\n",
      "  batch 12000 loss: 0.0001408849269595791\n",
      "  batch 12500 loss: 0.003732200332568901\n",
      "  batch 13000 loss: 4.71342389444942e-05\n",
      "  batch 13500 loss: 4.59368643948288e-05\n",
      "  batch 14000 loss: 5.411938755803547e-05\n",
      "  batch 14500 loss: 5.7158883535286977e-05\n",
      "  batch 15000 loss: 0.0005381323240048168\n",
      "  batch 15500 loss: 0.0001485674841974749\n",
      "  batch 16000 loss: 7.308156368696572e-05\n",
      "  batch 16500 loss: 3.7305052544560624e-05\n",
      "  batch 17000 loss: 0.0004258499134884097\n",
      "  batch 17500 loss: 0.009235776931961197\n",
      "  batch 18000 loss: 0.0001642169916444871\n",
      "  batch 18500 loss: 0.0009058906288410534\n",
      "  batch 19000 loss: 7.121973204759513e-05\n",
      "  batch 19500 loss: 9.134515847137337e-05\n",
      "  batch 20000 loss: 0.0005985829194310455\n",
      "  batch 20500 loss: 0.00017999534033479492\n",
      "  batch 21000 loss: 0.0004627037842446562\n",
      "  batch 21500 loss: 8.205037249614832e-05\n",
      "  batch 22000 loss: 5.834459090229061e-05\n",
      "  batch 22500 loss: 0.0032061039138403624\n",
      "  batch 23000 loss: 0.00010560371349050257\n",
      "  batch 23500 loss: 0.00014955067327571215\n",
      "  batch 24000 loss: 0.00023868815652675223\n",
      "  batch 24500 loss: 0.0002946191198011086\n",
      "  batch 25000 loss: 0.013280496287959749\n",
      "  batch 25500 loss: 0.00019398119078842057\n",
      "  batch 26000 loss: 0.000782397270721102\n",
      "  batch 26500 loss: 0.00015918895950824564\n",
      "  batch 27000 loss: 9.841904473002217e-05\n",
      "LOSS train 9.841904473002217e-05 valid 0.0015035352148782845\n",
      "EPOCH 49:\n",
      "  batch 500 loss: 0.00020417502080801242\n",
      "  batch 1000 loss: 0.0003083749549983601\n",
      "  batch 1500 loss: 0.00014090947328093505\n",
      "  batch 2000 loss: 6.52014848631417e-05\n",
      "  batch 2500 loss: 6.618187548137654e-05\n",
      "  batch 3000 loss: 4.6711801391726485e-05\n",
      "  batch 3500 loss: 0.00013537503195557222\n",
      "  batch 4000 loss: 5.240384608522319e-05\n",
      "  batch 4500 loss: 0.0034489264213241775\n",
      "  batch 5000 loss: 0.0002910383751319756\n",
      "  batch 5500 loss: 7.96457050010666e-05\n",
      "  batch 6000 loss: 0.007809529970451528\n",
      "  batch 6500 loss: 0.0003825006816710825\n",
      "  batch 7000 loss: 0.00013084406498837354\n",
      "  batch 7500 loss: 0.00023689235016699685\n",
      "  batch 8000 loss: 0.0001296489481900167\n",
      "  batch 8500 loss: 6.988592410835181e-05\n",
      "  batch 9000 loss: 8.788052654223221e-05\n",
      "  batch 9500 loss: 0.0014391663472194409\n",
      "  batch 10000 loss: 0.00011670157015342041\n",
      "  batch 10500 loss: 0.0015785722545726736\n",
      "  batch 11000 loss: 0.006027289041200565\n",
      "  batch 11500 loss: 0.00033716720619530706\n",
      "  batch 12000 loss: 0.00011906890469662911\n",
      "  batch 12500 loss: 0.0004810802684720059\n",
      "  batch 13000 loss: 0.0003306184618920405\n",
      "  batch 13500 loss: 0.0014226442536691302\n",
      "  batch 14000 loss: 0.0006579114342485966\n",
      "  batch 14500 loss: 0.0113443254735458\n",
      "  batch 15000 loss: 0.0008539763960598244\n",
      "  batch 15500 loss: 0.0015107358810464184\n",
      "  batch 16000 loss: 0.00015573387421829565\n",
      "  batch 16500 loss: 0.0005491308457093922\n",
      "  batch 17000 loss: 0.0005975706065107112\n",
      "  batch 17500 loss: 5.234551154070388e-05\n",
      "  batch 18000 loss: 0.0004033310986197058\n",
      "  batch 18500 loss: 0.00025007353642988404\n",
      "  batch 19000 loss: 0.0004066365723352554\n",
      "  batch 19500 loss: 0.00016770037098281777\n",
      "  batch 20000 loss: 6.513187967338752e-05\n",
      "  batch 20500 loss: 0.0005556691022943916\n",
      "  batch 21000 loss: 0.0002313732119466785\n",
      "  batch 21500 loss: 0.00011699638285391955\n",
      "  batch 22000 loss: 4.548097131864637e-05\n",
      "  batch 22500 loss: 0.0007608716258213448\n",
      "  batch 23000 loss: 0.00011769922391502163\n",
      "  batch 23500 loss: 0.00011118193401827981\n",
      "  batch 24000 loss: 6.604893788542299e-05\n",
      "  batch 24500 loss: 0.00013236306803523235\n",
      "  batch 25000 loss: 6.653582997460106e-05\n",
      "  batch 25500 loss: 6.944886758727264e-05\n",
      "  batch 26000 loss: 0.0001302042826831915\n",
      "  batch 26500 loss: 0.00019896696640012834\n",
      "  batch 27000 loss: 0.0009525151823960237\n",
      "LOSS train 0.0009525151823960237 valid 0.0012384836771667605\n",
      "EPOCH 50:\n",
      "  batch 500 loss: 0.0015288923184804055\n",
      "  batch 1000 loss: 0.0033349497463502614\n",
      "  batch 1500 loss: 0.00047094590004624235\n",
      "  batch 2000 loss: 0.00012470412884831462\n",
      "  batch 2500 loss: 4.5199232258315904e-05\n",
      "  batch 3000 loss: 0.00014878796162239637\n",
      "  batch 3500 loss: 0.0006117736456866822\n",
      "  batch 4000 loss: 0.0026721439239463826\n",
      "  batch 4500 loss: 0.0003100071729784659\n",
      "  batch 5000 loss: 4.99729834481073e-05\n",
      "  batch 5500 loss: 0.00035456332915772746\n",
      "  batch 6000 loss: 8.655469998365106e-05\n",
      "  batch 6500 loss: 8.662210096725432e-05\n",
      "  batch 7000 loss: 4.4069209228943864e-05\n",
      "  batch 7500 loss: 0.000220124095083559\n",
      "  batch 8000 loss: 0.00018569765948003435\n",
      "  batch 8500 loss: 0.0004908797235653282\n",
      "  batch 9000 loss: 0.005197888537732464\n",
      "  batch 9500 loss: 0.0013397946719600426\n",
      "  batch 10000 loss: 0.0001922935278545772\n",
      "  batch 10500 loss: 0.0002368162881245972\n",
      "  batch 11000 loss: 0.00011806766631527666\n",
      "  batch 11500 loss: 0.00014622667209993523\n",
      "  batch 12000 loss: 0.00016669388010692642\n",
      "  batch 12500 loss: 9.45918654043254e-05\n",
      "  batch 13000 loss: 0.0050339542050435995\n",
      "  batch 13500 loss: 5.800371812178895e-05\n",
      "  batch 14000 loss: 0.0004195088505780795\n",
      "  batch 14500 loss: 0.00010884682884952923\n",
      "  batch 15000 loss: 0.0003194852799348808\n",
      "  batch 15500 loss: 0.00010324553691337713\n",
      "  batch 16000 loss: 6.39857354415625e-05\n",
      "  batch 16500 loss: 4.644775801474665e-05\n",
      "  batch 17000 loss: 0.0001437971818967334\n",
      "  batch 17500 loss: 0.00024404585348288422\n",
      "  batch 18000 loss: 0.0009246226526909993\n",
      "  batch 18500 loss: 9.303684377595545e-05\n",
      "  batch 19000 loss: 9.176410655933865e-05\n",
      "  batch 19500 loss: 6.96368932529694e-05\n",
      "  batch 20000 loss: 0.00021262493698343476\n",
      "  batch 20500 loss: 0.002074717518399254\n",
      "  batch 21000 loss: 0.000948268331576351\n",
      "  batch 21500 loss: 0.0023235945291104334\n",
      "  batch 22000 loss: 0.0005989438933384363\n",
      "  batch 22500 loss: 0.04385658709466334\n",
      "  batch 23000 loss: 0.013516257610307125\n",
      "  batch 23500 loss: 0.0002034658265076139\n",
      "  batch 24000 loss: 0.002345727047236888\n",
      "  batch 24500 loss: 0.00027940631315034637\n",
      "  batch 25000 loss: 0.000307457836675205\n",
      "  batch 25500 loss: 0.0004260674828125879\n",
      "  batch 26000 loss: 0.003288058042745025\n",
      "  batch 26500 loss: 0.002592577374310267\n",
      "  batch 27000 loss: 0.00043011339333019196\n",
      "LOSS train 0.00043011339333019196 valid 0.0010382172513684355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Validation Loss')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "#This is doing some logging that we don't need to worry about right now.\n",
    "epoch_number = 0\n",
    "EPOCHS = 50\n",
    "best_vloss = 1_000_000.\n",
    "val_history = []\n",
    "best_model = model\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "    \n",
    "    model.train(True)\n",
    "    \n",
    "    avg_loss = train_one_epoch(curr_model=model)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    val_history.append(avg_vloss.detach().numpy())\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    \n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        torch.save(model, \"../models/model.pth\")\n",
    "    epoch_number += 1\n",
    "\n",
    "plt.plot(range(EPOCHS), val_history)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0010, dtype=torch.float64, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_vloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"../models/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct_count = 0\n",
    "total = len(test_set)\n",
    "with torch.no_grad():\n",
    "    for i, tdata in enumerate(test_loader):\n",
    "        inputs, labels = tdata\n",
    "        outputs = torch.argmax(model(inputs), dim=1)\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "        correct_count += (outputs==labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_count/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(np.argmax(model(test_data).detach().numpy(), axis=1), np.argmax(test_labels.detach().numpy(), axis=1), average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(train_data.view(-1,21*2), train_labels)\n",
    "y_pred = knn.predict(test_data.view(-1,21*2))\n",
    "accuracy_score(test_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1102, 21, 2])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7364130434782609"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RidgeClassifierCV()\n",
    "clf.fit(train_data.view(-1, 21*2), train_labels)\n",
    "y_pred = clf.predict(val_data.view(-1, 21*2))\n",
    "accuracy_score(val_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xw/slcw2lz14snfvxp49xgqmr880000gn/T/ipykernel_10494/1044316036.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  F.softmax(torch.tensor(clf.decision_function(val_data.view(-1, 21*2))))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0313, 0.0347, 0.0300,  ..., 0.0333, 0.0364, 0.0284],\n",
       "        [0.0302, 0.0299, 0.0406,  ..., 0.0398, 0.0329, 0.0293],\n",
       "        [0.0291, 0.0329, 0.0322,  ..., 0.0373, 0.0327, 0.0262],\n",
       "        ...,\n",
       "        [0.0321, 0.0456, 0.0328,  ..., 0.0455, 0.0309, 0.0308],\n",
       "        [0.0301, 0.0281, 0.0318,  ..., 0.0271, 0.0306, 0.0297],\n",
       "        [0.0308, 0.0270, 0.0341,  ..., 0.0257, 0.0334, 0.0357]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(torch.tensor(clf.decision_function(val_data.view(-1, 21*2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7364130434782609"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(val_data.view(-1, 21*2), val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reetinav/anaconda3/envs/PIC16B/lib/python3.9/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8888888888888888"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearSVC(multi_class=\"ovr\")\n",
    "clf.fit(train_data.view(-1, 21*2), torch.argmax(train_labels, dim=1))\n",
    "clf.score(val_data.view(-1, 21*2), torch.argmax(val_labels, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5952, 0.0090, 0.0528,  ..., 0.0620, 0.0913, 0.0809],\n",
       "        [0.5499, 0.0127, 0.0850,  ..., 0.0669, 0.0768, 0.0737],\n",
       "        [0.5842, 0.0089, 0.0531,  ..., 0.0594, 0.0923, 0.0878],\n",
       "        ...,\n",
       "        [0.0690, 0.1502, 0.1629,  ..., 0.1562, 0.0389, 0.0815],\n",
       "        [0.0903, 0.0719, 0.4556,  ..., 0.0353, 0.1396, 0.1260],\n",
       "        [0.0648, 0.0662, 0.5327,  ..., 0.0554, 0.0969, 0.1004]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(torch.tensor(clf.decision_function(val_data.view(-1, 21*2))), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PIC16B",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
